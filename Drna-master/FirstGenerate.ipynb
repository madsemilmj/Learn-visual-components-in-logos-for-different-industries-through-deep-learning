{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FirstGenerate.ipynb","provenance":[],"collapsed_sections":["ikEqI7vN2okD","tabvI9zw2lPe","ma2Qldo47KEm","d8K4sQEusecK","C_yuaoxjMn-d"],"authorship_tag":"ABX9TyNwZVVXi3ccOUhVpN+dSpzG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1532a0a2652340e49d39f36a1931e7b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e55444d542a04075a1115669411906bd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_356e0f1fc87243059c4196e0a70e7071","IPY_MODEL_cc04875a91a5417ab9db6006c9eb685f","IPY_MODEL_89ab31a55563457a878079ca4387be11"]}},"e55444d542a04075a1115669411906bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"356e0f1fc87243059c4196e0a70e7071":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9282c7bff5ff430b957ff08ee55088bb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_425462a1565e4e27a171a9b2363bdd20"}},"cc04875a91a5417ab9db6006c9eb685f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f9b870c3c88244c888e16080e4d8456e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":102530333,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102530333,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_80573294505e427793cf9a964fa7364b"}},"89ab31a55563457a878079ca4387be11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_164f5be9606c46f28f4fd93eaba54cd0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [00:00&lt;00:00, 123MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_38017d898589499ba6358be69ca40943"}},"9282c7bff5ff430b957ff08ee55088bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"425462a1565e4e27a171a9b2363bdd20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f9b870c3c88244c888e16080e4d8456e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"80573294505e427793cf9a964fa7364b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"164f5be9606c46f28f4fd93eaba54cd0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"38017d898589499ba6358be69ca40943":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"48efe4bfa58e420bbbf7a16036bba2e4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e3792a9bd1074b389cb058c67adfd8cf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_675fa136d55246029517bd129a4bda16","IPY_MODEL_1bf292c6925a4637ab4158786631074a","IPY_MODEL_3ebce76c7d9f47a9b40c8ecd903a84c8"]}},"e3792a9bd1074b389cb058c67adfd8cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"675fa136d55246029517bd129a4bda16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_49174972141646569da2dc702ab231a5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_61e343a1a80144d497d1ad7f718a9a9f"}},"1bf292c6925a4637ab4158786631074a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1aa5f0c4d02e47049d05ffb9392c3cdd","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":102530333,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102530333,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_af352415a4104dde975d79a6c0c48e9e"}},"3ebce76c7d9f47a9b40c8ecd903a84c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_17822f55f5e848c58a168c017cc81afd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [00:01&lt;00:00, 121MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9d6901a1ca2c4439aa4f1b993fe4f62d"}},"49174972141646569da2dc702ab231a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"61e343a1a80144d497d1ad7f718a9a9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1aa5f0c4d02e47049d05ffb9392c3cdd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"af352415a4104dde975d79a6c0c48e9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17822f55f5e848c58a168c017cc81afd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9d6901a1ca2c4439aa4f1b993fe4f62d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mDOdbok4zVE4"},"source":["# Colab Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GaPpoYZSzX_o","executionInfo":{"status":"ok","timestamp":1639097641923,"user_tz":-540,"elapsed":26592,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"8434a4d3-506f-43f6-f752-4265fbb51bbf"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"cSV78ENQzZvq","executionInfo":{"status":"ok","timestamp":1639097647284,"user_tz":-540,"elapsed":507,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"d63e4559-3f48-4191-f031-e56ae3b54fc4"},"source":["\"\"\"\n","Change directory to where this file is located\n","\"\"\"\n","#%cd '/content/drive/MyDrive/DLSP/Drna-master'"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nChange directory to where this file is located\\n'"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"ikEqI7vN2okD"},"source":["# Unzipping weights\n"]},{"cell_type":"code","metadata":{"id":"Q7ZGF7M42sLD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638794688607,"user_tz":-540,"elapsed":740,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"d41bc06f-04e4-47f6-8653-d3b76d4d561f"},"source":["zip_path_weights = \"/content/drive/MyDrive/DLSP/models/BestModelSoFar.zip\"\n","!cp \"{zip_path_weights}\" .\n","!unzip -q BestModelSoFar.zip\n","!rm BestModelSoFar.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/drive/MyDrive/DLSP/models/BestModelSoFar.zip': No such file or directory\n","unzip:  cannot find or open BestModelSoFar.zip, BestModelSoFar.zip.zip or BestModelSoFar.zip.ZIP.\n","rm: cannot remove 'BestModelSoFar.zip': No such file or directory\n"]}]},{"cell_type":"markdown","metadata":{"id":"tabvI9zw2lPe"},"source":["# Util functions"]},{"cell_type":"code","metadata":{"id":"AO_tGVq_hFgU","executionInfo":{"status":"ok","timestamp":1639097666876,"user_tz":-540,"elapsed":6250,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}}},"source":["import os\n","import copy\n","import numpy as np\n","from torch.autograd import Variable\n","import torch\n","from torch.optim import SGD\n","from torchvision import models\n","from torch import nn\n","from PIL import Image, ImageFilter\n","\n","def preprocess_image(pil_im, resize_im=True):\n","    \"\"\"\n","        Processes image for CNNs\n","    Args:\n","        PIL_img (PIL_img): PIL Image or numpy array to process\n","        resize_im (bool): Resize to 224 or not\n","    returns:\n","        im_as_var (torch variable): Variable that contains processed float tensor\n","    \"\"\"\n","    # mean and std list for channels (Imagenet)\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","\n","    #ensure or transform incoming image to PIL image\n","    if type(pil_im) != Image.Image:\n","        try:\n","            pil_im = Image.fromarray(pil_im)\n","        except Exception as e:\n","            print(\"could not transform PIL_img to a PIL Image object. Please check input.\")\n","\n","    # Resize image\n","    if resize_im:\n","        pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n","\n","    im_as_arr = np.float32(pil_im)\n","    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n","    # Normalize the channels\n","    for channel, _ in enumerate(im_as_arr):\n","        im_as_arr[channel] /= 255\n","        im_as_arr[channel] -= mean[channel]\n","        im_as_arr[channel] /= std[channel]\n","    # Convert to float tensor\n","    im_as_ten = torch.from_numpy(im_as_arr).float()\n","    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n","    im_as_ten.unsqueeze_(0)\n","    # Convert to Pytorch variable\n","    im_as_var = Variable(im_as_ten, requires_grad=True)\n","    return im_as_var\n","\n","def recreate_image(im_as_var):\n","    \"\"\"\n","        Recreates images from a torch variable, sort of reverse preprocessing\n","    Args:\n","        im_as_var (torch variable): Image to recreate\n","    returns:\n","        recreated_im (numpy arr): Recreated image in array\n","    \"\"\"\n","    reverse_mean = [-0.485, -0.456, -0.406]\n","    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n","    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n","    for c in range(3):\n","        recreated_im[c] /= reverse_std[c]\n","        recreated_im[c] -= reverse_mean[c]\n","    recreated_im[recreated_im > 1] = 1\n","    recreated_im[recreated_im < 0] = 0\n","    recreated_im = np.round(recreated_im * 255)\n","\n","    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n","    return recreated_im\n","\n","def save_image(im, path):\n","    \"\"\"\n","        Saves a numpy matrix or PIL image as an image\n","    Args:\n","        im_as_arr (Numpy array): Matrix of shape DxWxH\n","        path (str): Path to the image\n","    \"\"\"\n","    if torch.is_tensor(im):\n","        recreated_im = copy.copy(im.data.numpy()[0])\n","        im = np.uint8(recreated_im)\n","    if isinstance(im, (np.ndarray, np.generic)):\n","        im = format_np_output(im)\n","        im = Image.fromarray(im)\n","    im.save(path)\n","\n","def format_np_output(np_arr):\n","    \"\"\"\n","        This is a (kind of) bandaid fix to streamline saving procedure.\n","        It converts all the outputs to the same format which is 3xWxH\n","        with using sucecssive if clauses.\n","    Args:\n","        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n","    \"\"\"\n","    # Phase/Case 1: The np arr only has 2 dimensions\n","    # Result: Add a dimension at the beginning\n","    if len(np_arr.shape) == 2:\n","        np_arr = np.expand_dims(np_arr, axis=0)\n","    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n","    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n","    if np_arr.shape[0] == 1:\n","        np_arr = np.repeat(np_arr, 3, axis=0)\n","    # Phase/Case 3: Np arr is of shape 3xWxH\n","    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n","    if np_arr.shape[0] == 3:\n","        np_arr = np_arr.transpose(1, 2, 0)\n","    # Phase/Case 4: NP arr is normalized between 0-1\n","    # Result: Multiply with 255 and change type to make it saveable by PIL\n","    if np.max(np_arr) <= 1:\n","        np_arr = (np_arr*255).astype(np.uint8)\n","    return np_arr\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ma2Qldo47KEm"},"source":["# Non-Reguralized"]},{"cell_type":"code","metadata":{"id":"Y5OCeqCbenWF","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1638531823992,"user_tz":-540,"elapsed":7,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"aafe17a7-d49c-4d7b-ba33-805c6b03ee7a"},"source":["\"\"\"\n","Created on Thu Oct 26 14:19:44 2017\n","@author: Utku Ozbulak - github.com/utkuozbulak\n","\"\"\"\n","\n","class ClassSpecificImageGeneration():\n","    \"\"\"\n","        Produces an image that maximizes a certain class with gradient ascent\n","    \"\"\"\n","    def __init__(self, model, target_class):\n","        self.mean = [-0.485, -0.456, -0.406]\n","        self.std = [1/0.229, 1/0.224, 1/0.225]\n","        self.model = model\n","        self.model.eval()\n","        self.target_class = target_class\n","        # Generate a random image\n","        self.created_image = np.uint8(np.random.uniform(0, 255, (224, 224, 3)))\n","        # Create the folder to export images if not exists\n","        if not os.path.exists('../generated/class_'+str(self.target_class)):\n","            os.makedirs('../generated/class_'+str(self.target_class))\n","\n","    def generate(self, iterations=150):\n","        \"\"\"Generates class specific image\n","        Keyword Arguments:\n","            iterations {int} -- Total iterations for gradient ascent (default: {150})\n","        Returns:\n","            np.ndarray -- Final maximally activated class image\n","        \"\"\"\n","        initial_learning_rate = 6\n","        for i in range(1, iterations):\n","            # Process image and return variable\n","            self.processed_image = preprocess_image(self.created_image, False)\n","\n","            # Define optimizer for the image\n","            optimizer = SGD([self.processed_image], lr=initial_learning_rate)\n","            # Forward\n","            output = self.model(self.processed_image)\n","            # Target specific class\n","            class_loss = -output[0, self.target_class]\n","\n","            if i % 10 == 0 or i == iterations-1:\n","                print('Iteration:', str(i), 'Loss',\n","                      \"{0:.2f}\".format(class_loss.data.numpy()))\n","            # Zero grads\n","            self.model.zero_grad()\n","            # Backward\n","            class_loss.backward()\n","            # Update image\n","            optimizer.step()\n","            # Recreate image\n","            self.created_image = recreate_image(self.processed_image)\n","            if i % 10 == 0 or i == iterations-1:\n","                # Save image\n","                im_path = '/content/drive/MyDrive/DLSP/generated/class_'+str(self.target_class)+'/c_'+str(self.target_class)+'_'+'iter_'+str(i)+'.png'\n","                save_image(self.created_image, im_path)\n","\n","        return self.processed_image\n","\n","target_class = 2\n","model = models.resnet50(pretrained=True)\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 10)\n","######LOADING IN WEIGHTS#######\n","resume = \"\"\n","resume = \"./BestModelSoFar/006.ckpt\"\n","if resume:\n","    ckpt = torch.load(resume, map_location=torch.device('cpu'))\n","    model.load_state_dict(ckpt['net_state_dict'])\n","\n","\n","#target_class = 130  # Flamingo\n","#pretrained_model = models.alexnet(pretrained=True)\n","csig = ClassSpecificImageGeneration(model, target_class)\n","csig.generate()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-73a55571b83a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mtarget_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mnum_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"d8K4sQEusecK"},"source":["# Reguralized"]},{"cell_type":"code","metadata":{"id":"OgOyPEEtshcq","executionInfo":{"status":"ok","timestamp":1639006880318,"user_tz":-540,"elapsed":6239,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["1532a0a2652340e49d39f36a1931e7b6","e55444d542a04075a1115669411906bd","356e0f1fc87243059c4196e0a70e7071","cc04875a91a5417ab9db6006c9eb685f","89ab31a55563457a878079ca4387be11","9282c7bff5ff430b957ff08ee55088bb","425462a1565e4e27a171a9b2363bdd20","f9b870c3c88244c888e16080e4d8456e","80573294505e427793cf9a964fa7364b","164f5be9606c46f28f4fd93eaba54cd0","38017d898589499ba6358be69ca40943"]},"outputId":"58777b83-404b-470d-95a2-c60f1f67e42e"},"source":["use_cuda = torch.cuda.is_available()\n","\n","class RegularizedClassSpecificImageGeneration():\n","    \"\"\"\n","        Produces an image that maximizes a certain class with gradient ascent. Uses Gaussian blur, weight decay, and clipping. \n","    \"\"\"\n","\n","    def __init__(self, model, target_class):\n","        self.mean = [-0.485, -0.456, -0.406]\n","        self.std = [1/0.229, 1/0.224, 1/0.225]\n","        self.model = model.cuda() if use_cuda else model\n","        self.model.eval()\n","        self.target_class = target_class\n","        # Generate a random image\n","        self.created_image = np.uint8(np.random.uniform(0, 255, (224, 224, 3)))\n","        # Create the folder to export images if not exists\n","        if not os.path.exists(f'/content/drive/MyDrive/DLSP/generated_R/class_{self.target_class}'):\n","            os.makedirs(f'/content/drive/MyDrive/DLSP/generated_R/class_{self.target_class}')\n","\n","    def generate(self, iterations=150, blur_freq=4, blur_rad=1, wd=0.0001, clipping_value=0.1):\n","        \"\"\"Generates class specific image with enhancements to improve image quality. \n","        See https://arxiv.org/abs/1506.06579 for details on each argument's effect on output quality. \n","        \n","        Play around with combinations of arguments. Besides the defaults, this combination has produced good images:\n","        blur_freq=6, blur_rad=0.8, wd = 0.05\n","        Keyword Arguments:\n","            iterations {int} -- Total iterations for gradient ascent (default: {150})\n","            blur_freq {int} -- Frequency of Gaussian blur effect, in iterations (default: {6})\n","            blur_rad {float} -- Radius for gaussian blur, passed to PIL.ImageFilter.GaussianBlur() (default: {0.8})\n","            wd {float} -- Weight decay value for Stochastic Gradient Ascent (default: {0.05})\n","            clipping_value {None or float} -- Value for gradient clipping (default: {0.1})\n","        \n","        Returns:\n","            np.ndarray -- Final maximally activated class image\n","        \"\"\"\n","        initial_learning_rate = 6\n","        for i in range(1, iterations):\n","            # Process image and return variable\n","\n","            #implement gaussian blurring every ith iteration\n","            #to improve output\n","            if i % blur_freq == 0:\n","                self.processed_image = preprocess_and_blur_image(\n","                    self.created_image, False, blur_rad)\n","            else:\n","                self.processed_image = preprocess_and_blur_image(\n","                    self.created_image, False)\n","\n","            if use_cuda:\n","                self.processed_image = self.processed_image.cuda()\n","\n","            # Define optimizer for the image - use weight decay to add regularization\n","            # in SGD, wd = 2 * L2 regularization (https://bbabenko.github.io/weight-decay/)\n","            optimizer = SGD([self.processed_image],\n","                            lr=initial_learning_rate, weight_decay=wd)\n","            # Forward\n","            output = self.model(self.processed_image)\n","            # Target specific class\n","            class_loss = -output[0, self.target_class]\n","\n","            if i in np.linspace(0, iterations, 10, dtype=int):\n","                print('Iteration:', str(i), 'Loss',\n","                      \"{0:.2f}\".format(class_loss.data.cpu().numpy()))\n","            # Zero grads\n","            self.model.zero_grad()\n","            # Backward\n","            class_loss.backward()\n","\n","            if clipping_value:\n","                torch.nn.utils.clip_grad_norm(\n","                    self.model.parameters(), clipping_value)\n","            # Update image\n","            optimizer.step()\n","            # Recreate image\n","            self.created_image = recreate_image(self.processed_image.cpu())\n","\n","            if i in np.linspace(0, iterations, 10, dtype=int):\n","                # Save image\n","                im_path = f'/content/drive/MyDrive/DLSP/generated_R/class_{self.target_class}/c_{self.target_class}_iter_{i}_loss_{class_loss.data.cpu().numpy()}.jpg'\n","                save_image(self.created_image, im_path)\n","\n","        #save final image\n","        im_path = f'/content/drive/MyDrive/DLSP/generated_R/class_{self.target_class}/c_{self.target_class}_iter_{i}_loss_{class_loss.data.cpu().numpy()}.jpg'\n","        save_image(self.created_image, im_path)\n","\n","        #write file with regularization details\n","        with open(f'/content/drive/MyDrive/DLSP/generated_R/class_{self.target_class}/run_details.txt', 'w') as f:\n","            f.write(f'Iterations: {iterations}\\n')\n","            f.write(f'Blur freq: {blur_freq}\\n')\n","            f.write(f'Blur radius: {blur_rad}\\n')\n","            f.write(f'Weight decay: {wd}\\n')\n","            f.write(f'Clip value: {clipping_value}\\n')\n","\n","        #rename folder path with regularization details for easy access\n","        os.rename(f'/content/drive/MyDrive/DLSP/generated_R/class_{self.target_class}',\n","                  f'/content/drive/MyDrive/DLSP/generated_R/class_{self.target_class}_blurfreq_{blur_freq}_blurrad_{blur_rad}_wd_{wd}_CV{clipping_value}')\n","        return self.processed_image\n","\n","\n","def preprocess_and_blur_image(pil_im, resize_im=True, blur_rad=None):\n","    \"\"\"\n","        Processes image with optional Gaussian blur for CNNs\n","    Args:\n","        PIL_img (PIL_img): PIL Image or numpy array to process\n","        resize_im (bool): Resize to 224 or not\n","        blur_rad (int): Pixel radius for Gaussian blurring (default = None)\n","    returns:\n","        im_as_var (torch variable): Variable that contains processed float tensor\n","    \"\"\"\n","    # mean and std list for channels (Imagenet)\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","\n","    #ensure or transform incoming image to PIL image\n","    if type(pil_im) != Image.Image:\n","        try:\n","            pil_im = Image.fromarray(pil_im)\n","        except Exception as e:\n","            print(\n","                \"could not transform PIL_img to a PIL Image object. Please check input.\")\n","\n","    # Resize image\n","    if resize_im:\n","        pil_im.thumbnail((224, 224))\n","\n","    #add gaussin blur to image\n","    if blur_rad:\n","        pil_im = pil_im.filter(ImageFilter.GaussianBlur(blur_rad))\n","\n","    im_as_arr = np.float32(pil_im)\n","    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n","    # Normalize the channels\n","    for channel, _ in enumerate(im_as_arr):\n","        im_as_arr[channel] /= 255\n","        im_as_arr[channel] -= mean[channel]\n","        im_as_arr[channel] /= std[channel]\n","    # Convert to float tensor\n","    im_as_ten = torch.from_numpy(im_as_arr).float()\n","    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n","    im_as_ten.unsqueeze_(0)\n","    # Convert to Pytorch variable\n","    if use_cuda:\n","        im_as_var = Variable(im_as_ten.cuda(), requires_grad=True)\n","    else:\n","        im_as_var = Variable(im_as_ten, requires_grad=True)\n","    return im_as_var\n","\n","target_class = 0\n","model = models.resnet50(pretrained=True)\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 10)\n","######LOADING IN WEIGHTS#######\n","resume = \"\"\n","resume = \"/content/drive/MyDrive/DLSP/models/FinalModels/Train2/018.ckpt\"\n","if resume:\n","    ckpt = torch.load(resume, map_location=torch.device('cpu'))\n","    model.load_state_dict(ckpt['net_state_dict'])\n","#target_class = 130  # Flamingo\n","#pretrained_model = models.alexnet(pretrained=True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1532a0a2652340e49d39f36a1931e7b6","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"IAF24n7Dr-Lr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639019053571,"user_tz":-540,"elapsed":12166662,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"3f7eb74d-3339-4c78-b4dc-fea5b0f97d37"},"source":["for target_class in [4,7]:\n","  for blur_freq in [3,4,5,6,7]:\n","      for blur_rad in [1]:\n","          for wd in [0.1, 0.05, 0.005, 0.0001]:\n","              for clipping_value in [0.05, 0.1, 0.15]:\n","                  gen = RegularizedClassSpecificImageGeneration(model, target_class)\n","                  gen.generate(iterations=150, blur_freq=blur_freq, blur_rad=blur_rad, wd=wd, clipping_value=clipping_value)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:71: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"]},{"output_type":"stream","name":"stdout","text":["Iteration: 16 Loss -1.23\n","Iteration: 33 Loss -5.08\n","Iteration: 50 Loss -14.00\n","Iteration: 66 Loss -8.10\n","Iteration: 83 Loss -19.19\n","Iteration: 100 Loss -9.12\n","Iteration: 116 Loss -18.74\n","Iteration: 133 Loss -10.60\n","Iteration: 16 Loss -3.87\n","Iteration: 33 Loss -5.33\n","Iteration: 50 Loss -14.48\n","Iteration: 66 Loss -6.77\n","Iteration: 83 Loss -16.67\n","Iteration: 100 Loss -10.39\n","Iteration: 116 Loss -13.19\n","Iteration: 133 Loss -10.13\n","Iteration: 16 Loss -5.31\n","Iteration: 33 Loss -4.03\n","Iteration: 50 Loss -12.99\n","Iteration: 66 Loss -6.17\n","Iteration: 83 Loss -18.25\n","Iteration: 100 Loss -9.55\n","Iteration: 116 Loss -18.36\n","Iteration: 133 Loss -8.60\n","Iteration: 16 Loss -9.61\n","Iteration: 33 Loss -36.96\n","Iteration: 50 Loss -43.85\n","Iteration: 66 Loss -34.51\n","Iteration: 83 Loss -44.95\n","Iteration: 100 Loss -13.81\n","Iteration: 116 Loss -46.72\n","Iteration: 133 Loss -15.80\n","Iteration: 16 Loss -9.56\n","Iteration: 33 Loss -38.58\n","Iteration: 50 Loss -43.13\n","Iteration: 66 Loss -39.88\n","Iteration: 83 Loss -51.24\n","Iteration: 100 Loss -18.97\n","Iteration: 116 Loss -34.86\n","Iteration: 133 Loss -16.90\n","Iteration: 16 Loss -10.76\n","Iteration: 33 Loss -30.35\n","Iteration: 50 Loss -30.26\n","Iteration: 66 Loss -37.50\n","Iteration: 83 Loss -46.11\n","Iteration: 100 Loss -14.70\n","Iteration: 116 Loss -37.77\n","Iteration: 133 Loss -13.90\n","Iteration: 16 Loss -32.76\n","Iteration: 33 Loss -67.92\n","Iteration: 50 Loss -61.14\n","Iteration: 66 Loss -90.99\n","Iteration: 83 Loss -74.61\n","Iteration: 100 Loss -72.60\n","Iteration: 116 Loss -84.24\n","Iteration: 133 Loss -91.00\n","Iteration: 16 Loss -25.60\n","Iteration: 33 Loss -75.71\n","Iteration: 50 Loss -70.44\n","Iteration: 66 Loss -100.83\n","Iteration: 83 Loss -83.38\n","Iteration: 100 Loss -74.00\n","Iteration: 116 Loss -86.85\n","Iteration: 133 Loss -78.70\n","Iteration: 16 Loss -27.75\n","Iteration: 33 Loss -86.64\n","Iteration: 50 Loss -71.64\n","Iteration: 66 Loss -103.33\n","Iteration: 83 Loss -77.62\n","Iteration: 100 Loss -82.54\n","Iteration: 116 Loss -99.32\n","Iteration: 133 Loss -96.90\n","Iteration: 16 Loss -26.35\n","Iteration: 33 Loss -80.71\n","Iteration: 50 Loss -91.06\n","Iteration: 66 Loss -128.78\n","Iteration: 83 Loss -109.01\n","Iteration: 100 Loss -130.23\n","Iteration: 116 Loss -120.91\n","Iteration: 133 Loss -139.12\n","Iteration: 16 Loss -31.36\n","Iteration: 33 Loss -87.61\n","Iteration: 50 Loss -77.50\n","Iteration: 66 Loss -106.66\n","Iteration: 83 Loss -87.55\n","Iteration: 100 Loss -90.05\n","Iteration: 116 Loss -104.51\n","Iteration: 133 Loss -109.19\n","Iteration: 16 Loss -26.68\n","Iteration: 33 Loss -94.17\n","Iteration: 50 Loss -99.94\n","Iteration: 66 Loss -127.30\n","Iteration: 83 Loss -112.36\n","Iteration: 100 Loss -117.50\n","Iteration: 116 Loss -123.43\n","Iteration: 133 Loss -124.18\n","Iteration: 16 Loss -2.00\n","Iteration: 33 Loss -6.97\n","Iteration: 50 Loss -18.55\n","Iteration: 66 Loss -14.41\n","Iteration: 83 Loss -16.45\n","Iteration: 100 Loss -7.65\n","Iteration: 116 Loss -7.54\n","Iteration: 133 Loss -14.34\n","Iteration: 16 Loss -4.13\n","Iteration: 33 Loss -7.02\n","Iteration: 50 Loss -18.29\n","Iteration: 66 Loss -19.75\n","Iteration: 83 Loss -16.92\n","Iteration: 100 Loss -9.02\n","Iteration: 116 Loss -11.11\n","Iteration: 133 Loss -9.38\n","Iteration: 16 Loss -2.55\n","Iteration: 33 Loss -6.50\n","Iteration: 50 Loss -19.12\n","Iteration: 66 Loss -16.63\n","Iteration: 83 Loss -12.68\n","Iteration: 100 Loss -7.68\n","Iteration: 116 Loss -9.78\n","Iteration: 133 Loss -9.44\n","Iteration: 16 Loss -21.16\n","Iteration: 33 Loss -16.08\n","Iteration: 50 Loss -38.23\n","Iteration: 66 Loss -46.62\n","Iteration: 83 Loss -34.19\n","Iteration: 100 Loss -32.39\n","Iteration: 116 Loss -38.50\n","Iteration: 133 Loss -20.28\n","Iteration: 16 Loss -24.66\n","Iteration: 33 Loss -15.93\n","Iteration: 50 Loss -42.09\n","Iteration: 66 Loss -36.98\n","Iteration: 83 Loss -46.06\n","Iteration: 100 Loss -38.14\n","Iteration: 116 Loss -41.23\n","Iteration: 133 Loss -17.50\n","Iteration: 16 Loss -22.64\n","Iteration: 33 Loss -18.82\n","Iteration: 50 Loss -38.25\n","Iteration: 66 Loss -45.58\n","Iteration: 83 Loss -33.83\n","Iteration: 100 Loss -34.22\n","Iteration: 116 Loss -40.68\n","Iteration: 133 Loss -11.52\n","Iteration: 16 Loss -48.95\n","Iteration: 33 Loss -67.03\n","Iteration: 50 Loss -76.08\n","Iteration: 66 Loss -92.92\n","Iteration: 83 Loss -103.92\n","Iteration: 100 Loss -115.90\n","Iteration: 116 Loss -115.55\n","Iteration: 133 Loss -90.38\n","Iteration: 16 Loss -40.85\n","Iteration: 33 Loss -55.26\n","Iteration: 50 Loss -75.18\n","Iteration: 66 Loss -81.37\n","Iteration: 83 Loss -98.17\n","Iteration: 100 Loss -113.24\n","Iteration: 116 Loss -114.30\n","Iteration: 133 Loss -88.44\n","Iteration: 16 Loss -36.22\n","Iteration: 33 Loss -53.96\n","Iteration: 50 Loss -82.61\n","Iteration: 66 Loss -79.82\n","Iteration: 83 Loss -96.47\n","Iteration: 100 Loss -103.96\n","Iteration: 116 Loss -108.42\n","Iteration: 133 Loss -90.02\n","Iteration: 16 Loss -33.82\n","Iteration: 33 Loss -67.28\n","Iteration: 50 Loss -82.00\n","Iteration: 66 Loss -94.04\n","Iteration: 83 Loss -103.34\n","Iteration: 100 Loss -125.03\n","Iteration: 116 Loss -123.05\n","Iteration: 133 Loss -132.66\n","Iteration: 16 Loss -33.09\n","Iteration: 33 Loss -64.49\n","Iteration: 50 Loss -81.83\n","Iteration: 66 Loss -93.11\n","Iteration: 83 Loss -92.59\n","Iteration: 100 Loss -118.93\n","Iteration: 116 Loss -126.31\n","Iteration: 133 Loss -116.61\n","Iteration: 16 Loss -38.97\n","Iteration: 33 Loss -64.82\n","Iteration: 50 Loss -93.08\n","Iteration: 66 Loss -88.94\n","Iteration: 83 Loss -98.20\n","Iteration: 100 Loss -120.88\n","Iteration: 116 Loss -124.98\n","Iteration: 133 Loss -116.93\n","Iteration: 16 Loss -3.37\n","Iteration: 33 Loss -11.70\n","Iteration: 50 Loss -9.35\n","Iteration: 66 Loss -8.13\n","Iteration: 83 Loss -12.71\n","Iteration: 100 Loss -8.05\n","Iteration: 116 Loss -9.55\n","Iteration: 133 Loss -16.00\n","Iteration: 16 Loss -4.42\n","Iteration: 33 Loss -15.29\n","Iteration: 50 Loss -8.34\n","Iteration: 66 Loss -11.77\n","Iteration: 83 Loss -11.94\n","Iteration: 100 Loss -6.91\n","Iteration: 116 Loss -11.24\n","Iteration: 133 Loss -11.11\n","Iteration: 16 Loss -4.44\n","Iteration: 33 Loss -10.78\n","Iteration: 50 Loss -6.90\n","Iteration: 66 Loss -9.41\n","Iteration: 83 Loss -14.27\n","Iteration: 100 Loss -7.25\n","Iteration: 116 Loss -13.14\n","Iteration: 133 Loss -10.25\n","Iteration: 16 Loss -12.18\n","Iteration: 33 Loss -43.37\n","Iteration: 50 Loss -34.97\n","Iteration: 66 Loss -11.63\n","Iteration: 83 Loss -51.67\n","Iteration: 100 Loss -42.99\n","Iteration: 116 Loss -23.16\n","Iteration: 133 Loss -46.00\n","Iteration: 16 Loss -11.44\n","Iteration: 33 Loss -31.01\n","Iteration: 50 Loss -30.60\n","Iteration: 66 Loss -26.18\n","Iteration: 83 Loss -53.62\n","Iteration: 100 Loss -42.16\n","Iteration: 116 Loss -19.29\n","Iteration: 133 Loss -52.62\n","Iteration: 16 Loss -15.36\n","Iteration: 33 Loss -42.14\n","Iteration: 50 Loss -39.61\n","Iteration: 66 Loss -19.35\n","Iteration: 83 Loss -57.11\n","Iteration: 100 Loss -37.70\n","Iteration: 116 Loss -18.02\n","Iteration: 133 Loss -53.12\n","Iteration: 16 Loss -29.34\n","Iteration: 33 Loss -73.47\n","Iteration: 50 Loss -96.90\n","Iteration: 66 Loss -81.48\n","Iteration: 83 Loss -97.40\n","Iteration: 100 Loss -121.35\n","Iteration: 116 Loss -99.83\n","Iteration: 133 Loss -104.78\n","Iteration: 16 Loss -26.44\n","Iteration: 33 Loss -68.65\n","Iteration: 50 Loss -100.92\n","Iteration: 66 Loss -72.72\n","Iteration: 83 Loss -96.03\n","Iteration: 100 Loss -123.38\n","Iteration: 116 Loss -98.67\n","Iteration: 133 Loss -96.55\n","Iteration: 16 Loss -25.59\n","Iteration: 33 Loss -71.09\n","Iteration: 50 Loss -94.68\n","Iteration: 66 Loss -74.86\n","Iteration: 83 Loss -99.81\n","Iteration: 100 Loss -112.46\n","Iteration: 116 Loss -89.08\n","Iteration: 133 Loss -100.66\n","Iteration: 16 Loss -22.72\n","Iteration: 33 Loss -69.36\n","Iteration: 50 Loss -108.08\n","Iteration: 66 Loss -111.34\n","Iteration: 83 Loss -111.21\n","Iteration: 100 Loss -131.05\n","Iteration: 116 Loss -119.98\n","Iteration: 133 Loss -120.36\n","Iteration: 16 Loss -27.71\n","Iteration: 33 Loss -77.00\n","Iteration: 50 Loss -109.00\n","Iteration: 66 Loss -108.39\n","Iteration: 83 Loss -115.28\n","Iteration: 100 Loss -132.52\n","Iteration: 116 Loss -116.37\n","Iteration: 133 Loss -126.12\n","Iteration: 16 Loss -20.18\n","Iteration: 33 Loss -66.92\n","Iteration: 50 Loss -98.72\n","Iteration: 66 Loss -95.82\n","Iteration: 83 Loss -104.53\n","Iteration: 100 Loss -128.97\n","Iteration: 116 Loss -119.89\n","Iteration: 133 Loss -127.68\n","Iteration: 16 Loss -7.21\n","Iteration: 33 Loss -10.90\n","Iteration: 50 Loss -17.60\n","Iteration: 66 Loss -7.71\n","Iteration: 83 Loss -15.76\n","Iteration: 100 Loss -23.93\n","Iteration: 116 Loss -24.23\n","Iteration: 133 Loss -10.97\n","Iteration: 16 Loss -7.59\n","Iteration: 33 Loss -8.26\n","Iteration: 50 Loss -12.64\n","Iteration: 66 Loss -7.19\n","Iteration: 83 Loss -13.94\n","Iteration: 100 Loss -20.69\n","Iteration: 116 Loss -22.55\n","Iteration: 133 Loss -13.08\n","Iteration: 16 Loss -6.94\n","Iteration: 33 Loss -15.52\n","Iteration: 50 Loss -19.75\n","Iteration: 66 Loss -6.80\n","Iteration: 83 Loss -18.54\n","Iteration: 100 Loss -17.53\n","Iteration: 116 Loss -18.49\n","Iteration: 133 Loss -10.26\n","Iteration: 16 Loss -28.59\n","Iteration: 33 Loss -47.21\n","Iteration: 50 Loss -48.05\n","Iteration: 66 Loss -33.89\n","Iteration: 83 Loss -54.13\n","Iteration: 100 Loss -44.09\n","Iteration: 116 Loss -50.37\n","Iteration: 133 Loss -17.29\n","Iteration: 16 Loss -23.17\n","Iteration: 33 Loss -40.68\n","Iteration: 50 Loss -39.42\n","Iteration: 66 Loss -36.18\n","Iteration: 83 Loss -53.44\n","Iteration: 100 Loss -38.16\n","Iteration: 116 Loss -47.91\n","Iteration: 133 Loss -13.13\n","Iteration: 16 Loss -19.84\n","Iteration: 33 Loss -41.75\n","Iteration: 50 Loss -48.95\n","Iteration: 66 Loss -34.12\n","Iteration: 83 Loss -37.75\n","Iteration: 100 Loss -52.10\n","Iteration: 116 Loss -45.41\n","Iteration: 133 Loss -19.99\n","Iteration: 16 Loss -34.51\n","Iteration: 33 Loss -71.00\n","Iteration: 50 Loss -70.45\n","Iteration: 66 Loss -110.30\n","Iteration: 83 Loss -104.64\n","Iteration: 100 Loss -103.78\n","Iteration: 116 Loss -79.09\n","Iteration: 133 Loss -82.70\n","Iteration: 16 Loss -37.43\n","Iteration: 33 Loss -76.68\n","Iteration: 50 Loss -77.49\n","Iteration: 66 Loss -111.14\n","Iteration: 83 Loss -110.32\n","Iteration: 100 Loss -115.98\n","Iteration: 116 Loss -97.71\n","Iteration: 133 Loss -97.40\n","Iteration: 16 Loss -33.24\n","Iteration: 33 Loss -78.70\n","Iteration: 50 Loss -83.77\n","Iteration: 66 Loss -108.84\n","Iteration: 83 Loss -102.64\n","Iteration: 100 Loss -114.20\n","Iteration: 116 Loss -103.00\n","Iteration: 133 Loss -91.58\n","Iteration: 16 Loss -37.06\n","Iteration: 33 Loss -81.20\n","Iteration: 50 Loss -94.41\n","Iteration: 66 Loss -119.01\n","Iteration: 83 Loss -125.42\n","Iteration: 100 Loss -128.16\n","Iteration: 116 Loss -110.72\n","Iteration: 133 Loss -126.50\n","Iteration: 16 Loss -32.85\n","Iteration: 33 Loss -70.87\n","Iteration: 50 Loss -82.62\n","Iteration: 66 Loss -126.04\n","Iteration: 83 Loss -127.09\n","Iteration: 100 Loss -136.27\n","Iteration: 116 Loss -122.29\n","Iteration: 133 Loss -133.47\n","Iteration: 16 Loss -27.34\n","Iteration: 33 Loss -72.11\n","Iteration: 50 Loss -89.11\n","Iteration: 66 Loss -127.02\n","Iteration: 83 Loss -126.63\n","Iteration: 100 Loss -133.78\n","Iteration: 116 Loss -113.53\n","Iteration: 133 Loss -126.86\n","Iteration: 16 Loss -6.11\n","Iteration: 33 Loss -17.95\n","Iteration: 50 Loss -10.76\n","Iteration: 66 Loss -14.86\n","Iteration: 83 Loss -21.32\n","Iteration: 100 Loss -17.72\n","Iteration: 116 Loss -16.12\n","Iteration: 133 Loss -13.47\n","Iteration: 16 Loss -17.94\n","Iteration: 33 Loss -17.29\n","Iteration: 50 Loss -11.69\n","Iteration: 66 Loss -15.33\n","Iteration: 83 Loss -20.02\n","Iteration: 100 Loss -22.40\n","Iteration: 116 Loss -25.10\n","Iteration: 133 Loss -10.28\n","Iteration: 16 Loss -4.95\n","Iteration: 33 Loss -12.46\n","Iteration: 50 Loss -10.17\n","Iteration: 66 Loss -15.74\n","Iteration: 83 Loss -20.34\n","Iteration: 100 Loss -19.34\n","Iteration: 116 Loss -15.15\n","Iteration: 133 Loss -9.53\n","Iteration: 16 Loss -28.14\n","Iteration: 33 Loss -43.10\n","Iteration: 50 Loss -20.63\n","Iteration: 66 Loss -49.61\n","Iteration: 83 Loss -50.62\n","Iteration: 100 Loss -57.13\n","Iteration: 116 Loss -55.44\n","Iteration: 133 Loss -39.39\n","Iteration: 16 Loss -23.18\n","Iteration: 33 Loss -43.50\n","Iteration: 50 Loss -19.87\n","Iteration: 66 Loss -52.29\n","Iteration: 83 Loss -50.88\n","Iteration: 100 Loss -44.49\n","Iteration: 116 Loss -45.74\n","Iteration: 133 Loss -35.13\n","Iteration: 16 Loss -22.42\n","Iteration: 33 Loss -43.74\n","Iteration: 50 Loss -13.29\n","Iteration: 66 Loss -56.27\n","Iteration: 83 Loss -43.55\n","Iteration: 100 Loss -46.04\n","Iteration: 116 Loss -42.20\n","Iteration: 133 Loss -33.83\n","Iteration: 16 Loss -24.59\n","Iteration: 33 Loss -81.61\n","Iteration: 50 Loss -68.08\n","Iteration: 66 Loss -95.20\n","Iteration: 83 Loss -114.60\n","Iteration: 100 Loss -92.49\n","Iteration: 116 Loss -109.90\n","Iteration: 133 Loss -129.95\n","Iteration: 16 Loss -31.93\n","Iteration: 33 Loss -76.07\n","Iteration: 50 Loss -80.46\n","Iteration: 66 Loss -84.43\n","Iteration: 83 Loss -125.25\n","Iteration: 100 Loss -102.88\n","Iteration: 116 Loss -114.14\n","Iteration: 133 Loss -125.15\n","Iteration: 16 Loss -34.69\n","Iteration: 33 Loss -85.78\n","Iteration: 50 Loss -73.39\n","Iteration: 66 Loss -92.28\n","Iteration: 83 Loss -116.60\n","Iteration: 100 Loss -101.37\n","Iteration: 116 Loss -109.85\n","Iteration: 133 Loss -122.57\n","Iteration: 16 Loss -20.68\n","Iteration: 33 Loss -97.08\n","Iteration: 50 Loss -97.10\n","Iteration: 66 Loss -124.57\n","Iteration: 83 Loss -164.69\n","Iteration: 100 Loss -141.87\n","Iteration: 116 Loss -160.91\n","Iteration: 133 Loss -178.71\n","Iteration: 16 Loss -16.42\n","Iteration: 33 Loss -73.07\n","Iteration: 50 Loss -79.93\n","Iteration: 66 Loss -114.13\n","Iteration: 83 Loss -138.00\n","Iteration: 100 Loss -124.80\n","Iteration: 116 Loss -134.63\n","Iteration: 133 Loss -155.26\n","Iteration: 16 Loss -21.71\n","Iteration: 33 Loss -83.95\n","Iteration: 50 Loss -86.28\n","Iteration: 66 Loss -108.06\n","Iteration: 83 Loss -138.62\n","Iteration: 100 Loss -124.61\n","Iteration: 116 Loss -143.87\n","Iteration: 133 Loss -153.04\n","Iteration: 16 Loss 0.42\n","Iteration: 33 Loss -0.95\n","Iteration: 50 Loss -2.26\n","Iteration: 66 Loss -4.81\n","Iteration: 83 Loss 0.31\n","Iteration: 100 Loss -1.21\n","Iteration: 116 Loss -3.00\n","Iteration: 133 Loss -3.48\n","Iteration: 16 Loss 1.10\n","Iteration: 33 Loss 0.34\n","Iteration: 50 Loss -2.81\n","Iteration: 66 Loss 0.81\n","Iteration: 83 Loss 0.49\n","Iteration: 100 Loss -3.33\n","Iteration: 116 Loss -5.27\n","Iteration: 133 Loss 0.21\n","Iteration: 16 Loss 0.33\n","Iteration: 33 Loss -1.68\n","Iteration: 50 Loss -5.09\n","Iteration: 66 Loss -2.01\n","Iteration: 83 Loss -8.93\n","Iteration: 100 Loss -1.87\n","Iteration: 116 Loss -9.49\n","Iteration: 133 Loss -2.61\n","Iteration: 16 Loss -1.67\n","Iteration: 33 Loss -21.44\n","Iteration: 50 Loss -22.44\n","Iteration: 66 Loss -23.42\n","Iteration: 83 Loss -25.50\n","Iteration: 100 Loss -11.46\n","Iteration: 116 Loss -34.76\n","Iteration: 133 Loss -6.65\n","Iteration: 16 Loss -5.09\n","Iteration: 33 Loss -9.22\n","Iteration: 50 Loss -24.63\n","Iteration: 66 Loss -31.99\n","Iteration: 83 Loss -34.29\n","Iteration: 100 Loss -3.43\n","Iteration: 116 Loss -36.89\n","Iteration: 133 Loss -5.90\n","Iteration: 16 Loss 0.32\n","Iteration: 33 Loss -28.52\n","Iteration: 50 Loss -34.29\n","Iteration: 66 Loss -32.65\n","Iteration: 83 Loss -27.98\n","Iteration: 100 Loss -8.37\n","Iteration: 116 Loss -41.47\n","Iteration: 133 Loss -3.01\n","Iteration: 16 Loss -9.60\n","Iteration: 33 Loss -78.88\n","Iteration: 50 Loss -96.33\n","Iteration: 66 Loss -175.14\n","Iteration: 83 Loss -108.97\n","Iteration: 100 Loss -127.72\n","Iteration: 116 Loss -144.31\n","Iteration: 133 Loss -115.94\n","Iteration: 16 Loss -11.36\n","Iteration: 33 Loss -84.06\n","Iteration: 50 Loss -97.12\n","Iteration: 66 Loss -150.62\n","Iteration: 83 Loss -139.01\n","Iteration: 100 Loss -118.96\n","Iteration: 116 Loss -137.81\n","Iteration: 133 Loss -127.75\n","Iteration: 16 Loss -10.55\n","Iteration: 33 Loss -95.66\n","Iteration: 50 Loss -93.54\n","Iteration: 66 Loss -160.37\n","Iteration: 83 Loss -138.41\n","Iteration: 100 Loss -148.17\n","Iteration: 116 Loss -158.08\n","Iteration: 133 Loss -126.09\n","Iteration: 16 Loss -19.21\n","Iteration: 33 Loss -83.98\n","Iteration: 50 Loss -125.94\n","Iteration: 66 Loss -167.55\n","Iteration: 83 Loss -154.16\n","Iteration: 100 Loss -169.50\n","Iteration: 116 Loss -160.68\n","Iteration: 133 Loss -176.77\n","Iteration: 16 Loss -16.92\n","Iteration: 33 Loss -112.14\n","Iteration: 50 Loss -136.76\n","Iteration: 66 Loss -179.99\n","Iteration: 83 Loss -172.19\n","Iteration: 100 Loss -204.08\n","Iteration: 116 Loss -187.61\n","Iteration: 133 Loss -162.18\n","Iteration: 16 Loss -23.30\n","Iteration: 33 Loss -89.37\n","Iteration: 50 Loss -110.50\n","Iteration: 66 Loss -184.34\n","Iteration: 83 Loss -164.65\n","Iteration: 100 Loss -155.91\n","Iteration: 116 Loss -197.21\n","Iteration: 133 Loss -178.12\n","Iteration: 16 Loss -0.88\n","Iteration: 33 Loss -0.45\n","Iteration: 50 Loss -5.31\n","Iteration: 66 Loss -5.09\n","Iteration: 83 Loss -5.04\n","Iteration: 100 Loss -1.69\n","Iteration: 116 Loss -5.41\n","Iteration: 133 Loss -1.27\n","Iteration: 16 Loss 0.52\n","Iteration: 33 Loss -0.94\n","Iteration: 50 Loss -2.80\n","Iteration: 66 Loss -4.90\n","Iteration: 83 Loss 0.53\n","Iteration: 100 Loss -1.51\n","Iteration: 116 Loss -0.16\n","Iteration: 133 Loss 0.03\n","Iteration: 16 Loss -0.73\n","Iteration: 33 Loss -0.80\n","Iteration: 50 Loss -2.02\n","Iteration: 66 Loss -3.30\n","Iteration: 83 Loss -0.45\n","Iteration: 100 Loss -1.49\n","Iteration: 116 Loss -2.20\n","Iteration: 133 Loss 0.30\n","Iteration: 16 Loss -3.91\n","Iteration: 33 Loss -5.47\n","Iteration: 50 Loss -21.26\n","Iteration: 66 Loss -35.00\n","Iteration: 83 Loss -29.08\n","Iteration: 100 Loss -21.02\n","Iteration: 116 Loss -31.03\n","Iteration: 133 Loss -6.46\n","Iteration: 16 Loss -6.37\n","Iteration: 33 Loss -12.40\n","Iteration: 50 Loss -41.97\n","Iteration: 66 Loss -43.80\n","Iteration: 83 Loss -44.02\n","Iteration: 100 Loss -47.46\n","Iteration: 116 Loss -47.20\n","Iteration: 133 Loss -17.14\n","Iteration: 16 Loss -7.42\n","Iteration: 33 Loss -7.35\n","Iteration: 50 Loss -27.21\n","Iteration: 66 Loss -24.81\n","Iteration: 83 Loss -26.77\n","Iteration: 100 Loss -41.29\n","Iteration: 116 Loss -39.38\n","Iteration: 133 Loss -5.69\n","Iteration: 16 Loss -25.07\n","Iteration: 33 Loss -46.01\n","Iteration: 50 Loss -88.18\n","Iteration: 66 Loss -117.38\n","Iteration: 83 Loss -178.29\n","Iteration: 100 Loss -218.66\n","Iteration: 116 Loss -222.38\n","Iteration: 133 Loss -152.98\n","Iteration: 16 Loss -19.62\n","Iteration: 33 Loss -37.76\n","Iteration: 50 Loss -92.78\n","Iteration: 66 Loss -115.68\n","Iteration: 83 Loss -177.19\n","Iteration: 100 Loss -214.72\n","Iteration: 116 Loss -211.35\n","Iteration: 133 Loss -126.38\n","Iteration: 16 Loss -24.98\n","Iteration: 33 Loss -47.37\n","Iteration: 50 Loss -101.20\n","Iteration: 66 Loss -115.93\n","Iteration: 83 Loss -159.76\n","Iteration: 100 Loss -195.59\n","Iteration: 116 Loss -205.91\n","Iteration: 133 Loss -138.98\n","Iteration: 16 Loss -23.57\n","Iteration: 33 Loss -47.75\n","Iteration: 50 Loss -84.93\n","Iteration: 66 Loss -149.18\n","Iteration: 83 Loss -181.81\n","Iteration: 100 Loss -225.34\n","Iteration: 116 Loss -244.31\n","Iteration: 133 Loss -204.53\n","Iteration: 16 Loss -23.87\n","Iteration: 33 Loss -46.89\n","Iteration: 50 Loss -103.29\n","Iteration: 66 Loss -146.14\n","Iteration: 83 Loss -201.76\n","Iteration: 100 Loss -252.51\n","Iteration: 116 Loss -255.99\n","Iteration: 133 Loss -206.59\n","Iteration: 16 Loss -22.43\n","Iteration: 33 Loss -63.22\n","Iteration: 50 Loss -122.79\n","Iteration: 66 Loss -159.41\n","Iteration: 83 Loss -220.75\n","Iteration: 100 Loss -229.17\n","Iteration: 116 Loss -247.19\n","Iteration: 133 Loss -197.51\n","Iteration: 16 Loss 0.69\n","Iteration: 33 Loss -3.12\n","Iteration: 50 Loss -5.32\n","Iteration: 66 Loss -1.15\n","Iteration: 83 Loss 0.78\n","Iteration: 100 Loss -4.65\n","Iteration: 116 Loss -2.55\n","Iteration: 133 Loss -6.44\n","Iteration: 16 Loss 0.21\n","Iteration: 33 Loss -2.23\n","Iteration: 50 Loss -5.31\n","Iteration: 66 Loss -0.86\n","Iteration: 83 Loss -0.77\n","Iteration: 100 Loss -5.48\n","Iteration: 116 Loss -0.37\n","Iteration: 133 Loss -3.59\n","Iteration: 16 Loss 0.89\n","Iteration: 33 Loss -1.51\n","Iteration: 50 Loss -2.32\n","Iteration: 66 Loss -1.02\n","Iteration: 83 Loss -0.53\n","Iteration: 100 Loss 1.01\n","Iteration: 116 Loss -1.70\n","Iteration: 133 Loss -1.61\n","Iteration: 16 Loss -1.99\n","Iteration: 33 Loss -14.91\n","Iteration: 50 Loss -23.58\n","Iteration: 66 Loss -2.02\n","Iteration: 83 Loss -21.28\n","Iteration: 100 Loss -31.48\n","Iteration: 116 Loss -5.02\n","Iteration: 133 Loss -31.42\n","Iteration: 16 Loss -3.38\n","Iteration: 33 Loss -18.63\n","Iteration: 50 Loss -21.37\n","Iteration: 66 Loss -7.18\n","Iteration: 83 Loss -37.60\n","Iteration: 100 Loss -47.31\n","Iteration: 116 Loss -7.43\n","Iteration: 133 Loss -42.50\n","Iteration: 16 Loss -2.53\n","Iteration: 33 Loss -19.48\n","Iteration: 50 Loss -27.00\n","Iteration: 66 Loss -9.40\n","Iteration: 83 Loss -30.91\n","Iteration: 100 Loss -41.26\n","Iteration: 116 Loss -5.78\n","Iteration: 133 Loss -26.03\n","Iteration: 16 Loss -12.58\n","Iteration: 33 Loss -63.20\n","Iteration: 50 Loss -139.62\n","Iteration: 66 Loss -94.32\n","Iteration: 83 Loss -149.00\n","Iteration: 100 Loss -201.15\n","Iteration: 116 Loss -160.11\n","Iteration: 133 Loss -181.43\n","Iteration: 16 Loss -13.30\n","Iteration: 33 Loss -95.26\n","Iteration: 50 Loss -143.57\n","Iteration: 66 Loss -117.24\n","Iteration: 83 Loss -192.15\n","Iteration: 100 Loss -211.61\n","Iteration: 116 Loss -118.24\n","Iteration: 133 Loss -208.85\n","Iteration: 16 Loss -12.82\n","Iteration: 33 Loss -88.03\n","Iteration: 50 Loss -148.90\n","Iteration: 66 Loss -117.20\n","Iteration: 83 Loss -179.81\n","Iteration: 100 Loss -212.07\n","Iteration: 116 Loss -166.49\n","Iteration: 133 Loss -210.86\n","Iteration: 16 Loss -8.77\n","Iteration: 33 Loss -77.00\n","Iteration: 50 Loss -150.80\n","Iteration: 66 Loss -144.94\n","Iteration: 83 Loss -219.90\n","Iteration: 100 Loss -259.12\n","Iteration: 116 Loss -216.66\n","Iteration: 133 Loss -267.42\n","Iteration: 16 Loss -14.64\n","Iteration: 33 Loss -94.05\n","Iteration: 50 Loss -175.72\n","Iteration: 66 Loss -162.39\n","Iteration: 83 Loss -209.35\n","Iteration: 100 Loss -254.62\n","Iteration: 116 Loss -155.54\n","Iteration: 133 Loss -262.93\n","Iteration: 16 Loss -18.82\n","Iteration: 33 Loss -84.67\n","Iteration: 50 Loss -150.86\n","Iteration: 66 Loss -130.07\n","Iteration: 83 Loss -228.74\n","Iteration: 100 Loss -254.35\n","Iteration: 116 Loss -240.35\n","Iteration: 133 Loss -291.70\n","Iteration: 16 Loss -0.10\n","Iteration: 33 Loss -1.63\n","Iteration: 50 Loss -5.52\n","Iteration: 66 Loss -4.01\n","Iteration: 83 Loss -2.08\n","Iteration: 100 Loss -1.97\n","Iteration: 116 Loss -6.60\n","Iteration: 133 Loss -1.31\n","Iteration: 16 Loss 0.10\n","Iteration: 33 Loss -6.07\n","Iteration: 50 Loss -4.50\n","Iteration: 66 Loss -2.02\n","Iteration: 83 Loss -1.53\n","Iteration: 100 Loss -3.26\n","Iteration: 116 Loss -1.52\n","Iteration: 133 Loss -0.80\n","Iteration: 16 Loss 0.59\n","Iteration: 33 Loss -1.11\n","Iteration: 50 Loss -7.72\n","Iteration: 66 Loss -2.15\n","Iteration: 83 Loss -4.19\n","Iteration: 100 Loss -5.09\n","Iteration: 116 Loss -8.36\n","Iteration: 133 Loss -6.05\n","Iteration: 16 Loss -10.99\n","Iteration: 33 Loss -15.85\n","Iteration: 50 Loss -32.17\n","Iteration: 66 Loss -23.29\n","Iteration: 83 Loss -48.75\n","Iteration: 100 Loss -35.93\n","Iteration: 116 Loss -49.90\n","Iteration: 133 Loss -8.79\n","Iteration: 16 Loss -5.75\n","Iteration: 33 Loss -18.66\n","Iteration: 50 Loss -31.38\n","Iteration: 66 Loss -32.87\n","Iteration: 83 Loss -33.68\n","Iteration: 100 Loss -35.04\n","Iteration: 116 Loss -42.69\n","Iteration: 133 Loss -10.28\n","Iteration: 16 Loss -9.72\n","Iteration: 33 Loss -16.48\n","Iteration: 50 Loss -24.37\n","Iteration: 66 Loss -27.14\n","Iteration: 83 Loss -33.54\n","Iteration: 100 Loss -29.04\n","Iteration: 116 Loss -34.48\n","Iteration: 133 Loss -4.44\n","Iteration: 16 Loss -23.50\n","Iteration: 33 Loss -70.52\n","Iteration: 50 Loss -97.86\n","Iteration: 66 Loss -181.52\n","Iteration: 83 Loss -214.83\n","Iteration: 100 Loss -194.77\n","Iteration: 116 Loss -165.26\n","Iteration: 133 Loss -149.00\n","Iteration: 16 Loss -11.11\n","Iteration: 33 Loss -71.21\n","Iteration: 50 Loss -102.83\n","Iteration: 66 Loss -187.79\n","Iteration: 83 Loss -198.63\n","Iteration: 100 Loss -193.85\n","Iteration: 116 Loss -174.63\n","Iteration: 133 Loss -178.13\n","Iteration: 16 Loss -25.73\n","Iteration: 33 Loss -85.00\n","Iteration: 50 Loss -102.88\n","Iteration: 66 Loss -157.50\n","Iteration: 83 Loss -167.30\n","Iteration: 100 Loss -189.23\n","Iteration: 116 Loss -151.27\n","Iteration: 133 Loss -131.38\n","Iteration: 16 Loss -19.55\n","Iteration: 33 Loss -98.56\n","Iteration: 50 Loss -146.14\n","Iteration: 66 Loss -224.21\n","Iteration: 83 Loss -261.08\n","Iteration: 100 Loss -262.89\n","Iteration: 116 Loss -237.54\n","Iteration: 133 Loss -246.60\n","Iteration: 16 Loss -14.62\n","Iteration: 33 Loss -60.82\n","Iteration: 50 Loss -95.95\n","Iteration: 66 Loss -162.53\n","Iteration: 83 Loss -229.60\n","Iteration: 100 Loss -218.47\n","Iteration: 116 Loss -208.40\n","Iteration: 133 Loss -206.59\n","Iteration: 16 Loss -19.41\n","Iteration: 33 Loss -55.50\n","Iteration: 50 Loss -124.55\n","Iteration: 66 Loss -184.44\n","Iteration: 83 Loss -223.54\n","Iteration: 100 Loss -251.35\n","Iteration: 116 Loss -218.80\n","Iteration: 133 Loss -256.62\n","Iteration: 16 Loss -1.20\n","Iteration: 33 Loss -0.61\n","Iteration: 50 Loss -1.54\n","Iteration: 66 Loss -1.89\n","Iteration: 83 Loss -2.28\n","Iteration: 100 Loss -6.98\n","Iteration: 116 Loss -5.80\n","Iteration: 133 Loss -0.76\n","Iteration: 16 Loss -1.29\n","Iteration: 33 Loss -0.03\n","Iteration: 50 Loss -0.14\n","Iteration: 66 Loss 0.69\n","Iteration: 83 Loss -4.58\n","Iteration: 100 Loss -7.70\n","Iteration: 116 Loss -4.28\n","Iteration: 133 Loss -0.05\n","Iteration: 16 Loss 0.59\n","Iteration: 33 Loss -2.58\n","Iteration: 50 Loss -1.11\n","Iteration: 66 Loss -6.77\n","Iteration: 83 Loss -4.49\n","Iteration: 100 Loss -2.61\n","Iteration: 116 Loss -4.24\n","Iteration: 133 Loss 0.10\n","Iteration: 16 Loss -7.44\n","Iteration: 33 Loss -22.72\n","Iteration: 50 Loss -8.09\n","Iteration: 66 Loss -25.79\n","Iteration: 83 Loss -20.62\n","Iteration: 100 Loss -25.97\n","Iteration: 116 Loss -36.72\n","Iteration: 133 Loss -44.66\n","Iteration: 16 Loss -9.05\n","Iteration: 33 Loss -16.61\n","Iteration: 50 Loss -3.31\n","Iteration: 66 Loss -26.24\n","Iteration: 83 Loss -34.67\n","Iteration: 100 Loss -39.20\n","Iteration: 116 Loss -45.77\n","Iteration: 133 Loss -57.35\n","Iteration: 16 Loss -7.20\n","Iteration: 33 Loss -24.97\n","Iteration: 50 Loss -9.94\n","Iteration: 66 Loss -33.59\n","Iteration: 83 Loss -47.61\n","Iteration: 100 Loss -42.28\n","Iteration: 116 Loss -54.30\n","Iteration: 133 Loss -56.84\n","Iteration: 16 Loss -15.51\n","Iteration: 33 Loss -75.19\n","Iteration: 50 Loss -77.58\n","Iteration: 66 Loss -162.96\n","Iteration: 83 Loss -226.50\n","Iteration: 100 Loss -166.05\n","Iteration: 116 Loss -226.65\n","Iteration: 133 Loss -248.81\n","Iteration: 16 Loss -9.84\n","Iteration: 33 Loss -81.34\n","Iteration: 50 Loss -75.41\n","Iteration: 66 Loss -182.93\n","Iteration: 83 Loss -245.93\n","Iteration: 100 Loss -175.63\n","Iteration: 116 Loss -204.41\n","Iteration: 133 Loss -253.22\n","Iteration: 16 Loss -12.95\n","Iteration: 33 Loss -88.94\n","Iteration: 50 Loss -100.01\n","Iteration: 66 Loss -175.52\n","Iteration: 83 Loss -219.45\n","Iteration: 100 Loss -178.15\n","Iteration: 116 Loss -228.21\n","Iteration: 133 Loss -262.17\n","Iteration: 16 Loss -15.12\n","Iteration: 33 Loss -100.32\n","Iteration: 50 Loss -111.90\n","Iteration: 66 Loss -182.41\n","Iteration: 83 Loss -256.34\n","Iteration: 100 Loss -240.63\n","Iteration: 116 Loss -283.85\n","Iteration: 133 Loss -322.72\n","Iteration: 16 Loss -15.38\n","Iteration: 33 Loss -72.70\n","Iteration: 50 Loss -84.33\n","Iteration: 66 Loss -150.39\n","Iteration: 83 Loss -244.64\n","Iteration: 100 Loss -200.76\n","Iteration: 116 Loss -234.80\n","Iteration: 133 Loss -297.17\n","Iteration: 16 Loss -12.97\n","Iteration: 33 Loss -98.03\n","Iteration: 50 Loss -101.72\n","Iteration: 66 Loss -170.34\n","Iteration: 83 Loss -277.06\n","Iteration: 100 Loss -246.35\n","Iteration: 116 Loss -283.36\n","Iteration: 133 Loss -320.30\n"]}]},{"cell_type":"markdown","metadata":{"id":"C_yuaoxjMn-d"},"source":["# Reguralized with mean images"]},{"cell_type":"markdown","metadata":{"id":"ZWBPCkIj2EPQ"},"source":["## Loading meanimages"]},{"cell_type":"code","metadata":{"id":"YVGC_UcoMrGh"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","run_mean_images = []\n","combinations = [(\"Food\", 0), (\"Food\", 2), (\"Medical\", 1), (\"Medical\", 4)]\n","for comb in combinations:\n","  # Loading mean images\n","  stringer = \"/content/drive/MyDrive/DLSP/Drna-master/MeanImages/\"+comb[0]+\".npy\"\n","  MeanImagesAll = np.load(stringer, allow_pickle = True)\n","\n","  selected_cluster = comb[1]\n","  selected_mean_image = MeanImagesAll[selected_cluster,:,:]\n","  selected_mean_image.shape\n","\n","  #Reshaping into right shape\n","  selected_mean_image_raw = np.swapaxes(np.swapaxes(selected_mean_image,0,1),1,2)\n","  # Rescaling\n","  selected_mean_image = np.clip(selected_mean_image_raw,0,1)\n","  run_mean_images.append(selected_mean_image)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QhENity72IZe"},"source":["## Generating images"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["48efe4bfa58e420bbbf7a16036bba2e4","e3792a9bd1074b389cb058c67adfd8cf","675fa136d55246029517bd129a4bda16","1bf292c6925a4637ab4158786631074a","3ebce76c7d9f47a9b40c8ecd903a84c8","49174972141646569da2dc702ab231a5","61e343a1a80144d497d1ad7f718a9a9f","1aa5f0c4d02e47049d05ffb9392c3cdd","af352415a4104dde975d79a6c0c48e9e","17822f55f5e848c58a168c017cc81afd","9d6901a1ca2c4439aa4f1b993fe4f62d"]},"id":"d9TDOu3VNJxg","executionInfo":{"status":"ok","timestamp":1639049960413,"user_tz":-540,"elapsed":4149,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"f194e7d4-0708-4069-d2af-7c01f8895c62"},"source":["use_cuda = torch.cuda.is_available()\n","\n","class RegularizedClassSpecificImageGeneration():\n","    \"\"\"\n","        Produces an image that maximizes a certain class with gradient ascent. Uses Gaussian blur, weight decay, and clipping. \n","    \"\"\"\n","\n","    def __init__(self, model, target_class, selected_mean_image, cluster):\n","        self.mean = [-0.485, -0.456, -0.406]\n","        self.std = [1/0.229, 1/0.224, 1/0.225]\n","        self.model = model.cuda() if use_cuda else model\n","        self.model.eval()\n","        self.target_class = target_class\n","        self.cluster = cluster\n","        # Generate a random image\n","        self.created_image = np.uint8(np.random.uniform(0, 255, (224, 224, 3)))\n","        self.created_image = np.uint8(selected_mean_image*255)\n","        # Create the folder to export images if not exists\n","        if not os.path.exists(f'/content/drive/MyDrive/DLSP/generated_R1/class_{self.target_class}'):\n","            os.makedirs(f'/content/drive/MyDrive/DLSP/generated_R1/class_{self.target_class}')\n","\n","    def generate(self, iterations=150, blur_freq=4, blur_rad=1, wd=0.01, clipping_value=0.1):\n","        \"\"\"Generates class specific image with enhancements to improve image quality. \n","        See https://arxiv.org/abs/1506.06579 for details on each argument's effect on output quality. \n","        #ORIGINAL iterations=150, blur_freq=4, blur_rad=1, wd=0.0001, clipping_value=0.1\n","        Play around with combinations of arguments. Besides the defaults, this combination has produced good images:\n","        blur_freq=6, blur_rad=0.8, wd = 0.05\n","        Keyword Arguments:\n","            iterations {int} -- Total iterations for gradient ascent (default: {150})\n","            blur_freq {int} -- Frequency of Gaussian blur effect, in iterations (default: {6})\n","            blur_rad {float} -- Radius for gaussian blur, passed to PIL.ImageFilter.GaussianBlur() (default: {0.8})\n","            wd {float} -- Weight decay value for Stochastic Gradient Ascent (default: {0.05})\n","            clipping_value {None or float} -- Value for gradient clipping (default: {0.1})\n","        \n","        Returns:\n","            np.ndarray -- Final maximally activated class image\n","        \"\"\"\n","        initial_learning_rate = 6\n","        for i in range(1, iterations):\n","            # Process image and return variable\n","\n","            #implement gaussian blurring every ith iteration\n","            #to improve output\n","            if i % blur_freq == 0:\n","                self.processed_image = preprocess_and_blur_image(\n","                    self.created_image, False, blur_rad)\n","            else:\n","                self.processed_image = preprocess_and_blur_image(\n","                    self.created_image, False)\n","\n","            if use_cuda:\n","                self.processed_image = self.processed_image.cuda()\n","\n","            # Define optimizer for the image - use weight decay to add regularization\n","            # in SGD, wd = 2 * L2 regularization (https://bbabenko.github.io/weight-decay/)\n","            optimizer = SGD([self.processed_image],\n","                            lr=initial_learning_rate, weight_decay=wd)\n","            # Forward\n","            output = self.model(self.processed_image)\n","            # Target specific class\n","            class_loss = -output[0, self.target_class]\n","\n","            if i in np.linspace(0, iterations, 10, dtype=int):\n","                print('Iteration:', str(i), 'Loss',\n","                      \"{0:.2f}\".format(class_loss.data.cpu().numpy()))\n","            # Zero grads\n","            self.model.zero_grad()\n","            # Backward\n","            class_loss.backward()\n","\n","            if clipping_value:\n","                torch.nn.utils.clip_grad_norm(\n","                    self.model.parameters(), clipping_value)\n","            # Update image\n","            optimizer.step()\n","            # Recreate image\n","            self.created_image = recreate_image(self.processed_image.cpu())\n","\n","            if i in np.linspace(0, iterations, 10, dtype=int):\n","                # Save image\n","                im_path = f'/content/drive/MyDrive/DLSP/generated_R1/class_{self.target_class}/c_{self.target_class}_iter_{i}_loss_{class_loss.data.cpu().numpy()}.jpg'\n","                save_image(self.created_image, im_path)\n","\n","        #save final image\n","        im_path = f'/content/drive/MyDrive/DLSP/generated_R1/class_{self.target_class}/c_{self.target_class}_iter_{i}_loss_{class_loss.data.cpu().numpy()}.jpg'\n","        save_image(self.created_image, im_path)\n","\n","        #write file with regularization details\n","        with open(f'/content/drive/MyDrive/DLSP/generated_R1/class_{self.target_class}/run_details.txt', 'w') as f:\n","            f.write(f'Iterations: {iterations}\\n')\n","            f.write(f'Blur freq: {blur_freq}\\n')\n","            f.write(f'Blur radius: {blur_rad}\\n')\n","            f.write(f'Weight decay: {wd}\\n')\n","            f.write(f'Clip value: {clipping_value}\\n')\n","\n","        #rename folder path with regularization details for easy access\n","        os.rename(f'/content/drive/MyDrive/DLSP/generated_R1/class_{self.target_class}',\n","                  f'/content/drive/MyDrive/DLSP/generated_R1/class_{self.target_class}_blurfreq_{blur_freq}_blurrad_{blur_rad}_wd{wd}_CV{clipping_value}_cluster_{self.cluster}')\n","        return self.processed_image\n","\n","\n","def preprocess_and_blur_image(pil_im, resize_im=True, blur_rad=None):\n","    \"\"\"\n","        Processes image with optional Gaussian blur for CNNs\n","    Args:\n","        PIL_img (PIL_img): PIL Image or numpy array to process\n","        resize_im (bool): Resize to 224 or not\n","        blur_rad (int): Pixel radius for Gaussian blurring (default = None)\n","    returns:\n","        im_as_var (torch variable): Variable that contains processed float tensor\n","    \"\"\"\n","    # mean and std list for channels (Imagenet)\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","\n","    #ensure or transform incoming image to PIL image\n","    if type(pil_im) != Image.Image:\n","        try:\n","            pil_im = Image.fromarray(pil_im)\n","        except Exception as e:\n","            print(\n","                \"could not transform PIL_img to a PIL Image object. Please check input.\")\n","\n","    # Resize image\n","    if resize_im:\n","        pil_im.thumbnail((224, 224))\n","\n","    #add gaussin blur to image\n","    if blur_rad:\n","        pil_im = pil_im.filter(ImageFilter.GaussianBlur(blur_rad))\n","\n","    im_as_arr = np.float32(pil_im)\n","    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n","    # Normalize the channels\n","    for channel, _ in enumerate(im_as_arr):\n","        im_as_arr[channel] /= 255\n","        im_as_arr[channel] -= mean[channel]\n","        im_as_arr[channel] /= std[channel]\n","    # Convert to float tensor\n","    im_as_ten = torch.from_numpy(im_as_arr).float()\n","    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n","    im_as_ten.unsqueeze_(0)\n","    # Convert to Pytorch variable\n","    if use_cuda:\n","        im_as_var = Variable(im_as_ten.cuda(), requires_grad=True)\n","    else:\n","        im_as_var = Variable(im_as_ten, requires_grad=True)\n","    return im_as_var\n","\n","target_class = 7\n","model = models.resnet50(pretrained=True)\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 10)\n","######LOADING IN WEIGHTS#######\n","resume = \"\"\n","resume = \"/content/drive/MyDrive/DLSP/models/FinalModels/Train2/018.ckpt\"\n","if resume:\n","    ckpt = torch.load(resume, map_location=torch.device('cpu'))\n","    model.load_state_dict(ckpt['net_state_dict'])\n","#target_class = 130  # Flamingo\n","#pretrained_model = models.alexnet(pretrained=True)\n","#csig = RegularizedClassSpecificImageGeneration(model, target_class, selected_mean_image)\n","#csig.generate()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48efe4bfa58e420bbbf7a16036bba2e4","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","source":["combinations[2:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAbPJ0NRztlT","executionInfo":{"status":"ok","timestamp":1639046906446,"user_tz":-540,"elapsed":304,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"d3a14476-ba14-4a77-f919-85f905d3a152"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Medical', 1), ('Medical', 4)]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["for i,comb in enumerate(combinations[3:]):\n","  if comb[0] == \"Food\":\n","    target_class = 4\n","  else:\n","    target_class = 7\n","  for blur_freq in [3,4,5,6,7]:\n","      for blur_rad in [1]:\n","          for wd in [0.1, 0.05, 0.005, 0.0001]:\n","              for clipping_value in [0.05, 0.1, 0.15]:\n","                  gen = RegularizedClassSpecificImageGeneration(model, target_class, run_mean_images[i], comb[1])\n","                  gen.generate(iterations=150, blur_freq=blur_freq, blur_rad=blur_rad, wd=wd, clipping_value=clipping_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"dZe3_3zIGoo3","executionInfo":{"status":"error","timestamp":1639053026696,"user_tz":-540,"elapsed":3058596,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"e5bc2463-8cea-4338-b09a-67357d392107"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"]},{"output_type":"stream","name":"stdout","text":["Iteration: 16 Loss -1.84\n","Iteration: 33 Loss -0.32\n","Iteration: 50 Loss -3.62\n","Iteration: 66 Loss -0.18\n","Iteration: 83 Loss -1.61\n","Iteration: 100 Loss 0.25\n","Iteration: 116 Loss -4.68\n","Iteration: 133 Loss 0.68\n","Iteration: 16 Loss -1.84\n","Iteration: 33 Loss -0.32\n","Iteration: 50 Loss -3.62\n","Iteration: 66 Loss -0.18\n","Iteration: 83 Loss -1.61\n","Iteration: 100 Loss 0.25\n","Iteration: 116 Loss -4.68\n","Iteration: 133 Loss 0.68\n","Iteration: 16 Loss -1.84\n","Iteration: 33 Loss -0.32\n","Iteration: 50 Loss -3.62\n","Iteration: 66 Loss -0.18\n","Iteration: 83 Loss -1.61\n","Iteration: 100 Loss 0.25\n","Iteration: 116 Loss -4.68\n","Iteration: 133 Loss 0.68\n","Iteration: 16 Loss -2.31\n","Iteration: 33 Loss -26.73\n","Iteration: 50 Loss -26.65\n","Iteration: 66 Loss -31.77\n","Iteration: 83 Loss -21.95\n","Iteration: 100 Loss -11.11\n","Iteration: 116 Loss -41.65\n","Iteration: 133 Loss -15.26\n","Iteration: 16 Loss -2.31\n","Iteration: 33 Loss -26.73\n","Iteration: 50 Loss -26.65\n","Iteration: 66 Loss -31.77\n","Iteration: 83 Loss -21.95\n","Iteration: 100 Loss -11.11\n","Iteration: 116 Loss -41.65\n","Iteration: 133 Loss -15.26\n","Iteration: 16 Loss -2.31\n","Iteration: 33 Loss -26.73\n","Iteration: 50 Loss -26.65\n","Iteration: 66 Loss -31.77\n","Iteration: 83 Loss -21.95\n","Iteration: 100 Loss -11.11\n","Iteration: 116 Loss -41.65\n","Iteration: 133 Loss -15.26\n","Iteration: 16 Loss -63.18\n","Iteration: 33 Loss -180.66\n","Iteration: 50 Loss -112.33\n","Iteration: 66 Loss -199.65\n","Iteration: 83 Loss -148.58\n","Iteration: 100 Loss -123.09\n","Iteration: 116 Loss -129.54\n","Iteration: 133 Loss -116.67\n","Iteration: 16 Loss -63.18\n","Iteration: 33 Loss -180.66\n","Iteration: 50 Loss -112.33\n","Iteration: 66 Loss -199.65\n","Iteration: 83 Loss -148.58\n","Iteration: 100 Loss -123.09\n","Iteration: 116 Loss -129.54\n","Iteration: 133 Loss -116.67\n","Iteration: 16 Loss -63.18\n","Iteration: 33 Loss -180.66\n","Iteration: 50 Loss -112.33\n","Iteration: 66 Loss -199.65\n","Iteration: 83 Loss -148.58\n","Iteration: 100 Loss -123.09\n","Iteration: 116 Loss -129.54\n","Iteration: 133 Loss -116.67\n","Iteration: 16 Loss -71.32\n","Iteration: 33 Loss -185.89\n","Iteration: 50 Loss -143.16\n","Iteration: 66 Loss -238.67\n","Iteration: 83 Loss -194.83\n","Iteration: 100 Loss -166.56\n","Iteration: 116 Loss -211.01\n","Iteration: 133 Loss -196.16\n","Iteration: 16 Loss -71.32\n","Iteration: 33 Loss -185.89\n","Iteration: 50 Loss -143.16\n","Iteration: 66 Loss -238.67\n","Iteration: 83 Loss -194.83\n","Iteration: 100 Loss -166.56\n","Iteration: 116 Loss -211.01\n","Iteration: 133 Loss -196.16\n","Iteration: 16 Loss -71.32\n","Iteration: 33 Loss -185.89\n","Iteration: 50 Loss -143.16\n","Iteration: 66 Loss -238.67\n","Iteration: 83 Loss -194.83\n","Iteration: 100 Loss -166.56\n","Iteration: 116 Loss -211.01\n","Iteration: 133 Loss -196.16\n","Iteration: 16 Loss -3.07\n","Iteration: 33 Loss -0.07\n","Iteration: 50 Loss -4.30\n","Iteration: 66 Loss -3.32\n","Iteration: 83 Loss -4.47\n","Iteration: 100 Loss -4.94\n","Iteration: 116 Loss 0.72\n","Iteration: 133 Loss -0.78\n","Iteration: 16 Loss -3.07\n","Iteration: 33 Loss -0.07\n","Iteration: 50 Loss -4.30\n","Iteration: 66 Loss -3.32\n","Iteration: 83 Loss -4.47\n","Iteration: 100 Loss -4.94\n","Iteration: 116 Loss 0.72\n","Iteration: 133 Loss -0.78\n","Iteration: 16 Loss -3.07\n","Iteration: 33 Loss -0.07\n","Iteration: 50 Loss -4.30\n","Iteration: 66 Loss -3.32\n","Iteration: 83 Loss -4.47\n","Iteration: 100 Loss -4.94\n","Iteration: 116 Loss 0.72\n","Iteration: 133 Loss -0.78\n","Iteration: 16 Loss -29.70\n","Iteration: 33 Loss -13.47\n","Iteration: 50 Loss -37.67\n","Iteration: 66 Loss -44.82\n","Iteration: 83 Loss -39.81\n","Iteration: 100 Loss -19.71\n","Iteration: 116 Loss -34.29\n","Iteration: 133 Loss -14.34\n","Iteration: 16 Loss -29.70\n","Iteration: 33 Loss -13.47\n","Iteration: 50 Loss -37.67\n","Iteration: 66 Loss -44.82\n","Iteration: 83 Loss -39.81\n","Iteration: 100 Loss -19.71\n","Iteration: 116 Loss -34.29\n","Iteration: 133 Loss -14.34\n","Iteration: 16 Loss -29.70\n","Iteration: 33 Loss -13.47\n","Iteration: 50 Loss -37.67\n","Iteration: 66 Loss -44.82\n","Iteration: 83 Loss -39.81\n","Iteration: 100 Loss -19.71\n","Iteration: 116 Loss -34.29\n","Iteration: 133 Loss -14.34\n","Iteration: 16 Loss -142.94\n","Iteration: 33 Loss -91.30\n","Iteration: 50 Loss -136.58\n","Iteration: 66 Loss -164.50\n","Iteration: 83 Loss -150.76\n","Iteration: 100 Loss -204.81\n","Iteration: 116 Loss -232.22\n","Iteration: 133 Loss -157.86\n","Iteration: 16 Loss -142.94\n","Iteration: 33 Loss -91.30\n","Iteration: 50 Loss -136.58\n","Iteration: 66 Loss -164.50\n","Iteration: 83 Loss -150.76\n","Iteration: 100 Loss -204.81\n","Iteration: 116 Loss -232.22\n","Iteration: 133 Loss -157.86\n","Iteration: 16 Loss -142.94\n","Iteration: 33 Loss -91.30\n","Iteration: 50 Loss -136.58\n","Iteration: 66 Loss -164.50\n","Iteration: 83 Loss -150.76\n","Iteration: 100 Loss -204.81\n","Iteration: 116 Loss -232.22\n","Iteration: 133 Loss -157.86\n","Iteration: 16 Loss -115.89\n","Iteration: 33 Loss -114.91\n","Iteration: 50 Loss -148.93\n","Iteration: 66 Loss -187.47\n","Iteration: 83 Loss -196.89\n","Iteration: 100 Loss -283.68\n","Iteration: 116 Loss -302.35\n","Iteration: 133 Loss -234.48\n","Iteration: 16 Loss -115.89\n","Iteration: 33 Loss -114.91\n","Iteration: 50 Loss -148.93\n","Iteration: 66 Loss -187.47\n","Iteration: 83 Loss -196.89\n","Iteration: 100 Loss -283.68\n","Iteration: 116 Loss -302.35\n","Iteration: 133 Loss -234.48\n","Iteration: 16 Loss -115.89\n","Iteration: 33 Loss -114.91\n","Iteration: 50 Loss -148.93\n","Iteration: 66 Loss -187.47\n","Iteration: 83 Loss -196.89\n","Iteration: 100 Loss -283.68\n","Iteration: 116 Loss -302.35\n","Iteration: 133 Loss -234.48\n","Iteration: 16 Loss 0.49\n","Iteration: 33 Loss -6.08\n","Iteration: 50 Loss 0.32\n","Iteration: 66 Loss -2.36\n","Iteration: 83 Loss -3.62\n","Iteration: 100 Loss -3.62\n","Iteration: 116 Loss -1.76\n","Iteration: 133 Loss 0.99\n","Iteration: 16 Loss 0.49\n","Iteration: 33 Loss -6.08\n","Iteration: 50 Loss 0.32\n","Iteration: 66 Loss -2.36\n","Iteration: 83 Loss -3.62\n","Iteration: 100 Loss -3.62\n","Iteration: 116 Loss -1.76\n","Iteration: 133 Loss 0.99\n","Iteration: 16 Loss 0.49\n","Iteration: 33 Loss -6.08\n","Iteration: 50 Loss 0.32\n","Iteration: 66 Loss -2.36\n","Iteration: 83 Loss -3.62\n","Iteration: 100 Loss -3.62\n","Iteration: 116 Loss -1.76\n","Iteration: 133 Loss 0.99\n","Iteration: 16 Loss -2.91\n","Iteration: 33 Loss -36.49\n","Iteration: 50 Loss -43.97\n","Iteration: 66 Loss -10.22\n","Iteration: 83 Loss -41.26\n","Iteration: 100 Loss -32.42\n","Iteration: 116 Loss -14.30\n","Iteration: 133 Loss -44.65\n","Iteration: 16 Loss -2.91\n","Iteration: 33 Loss -36.49\n","Iteration: 50 Loss -43.97\n","Iteration: 66 Loss -10.22\n","Iteration: 83 Loss -41.26\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-da5ed9c45b09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mclipping_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegularizedClassSpecificImageGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_mean_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                   \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblur_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblur_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblur_rad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblur_rad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclipping_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-755827661f28>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, iterations, blur_freq, blur_rad, wd, clipping_value)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mclass_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclipping_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"bUkKrzj3iFH7"},"source":["https://github.com/utkuozbulak/pytorch-cnn-visualizations/tree/master/src"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vd8OVQio2M-q"},"source":["# Reguralized with GAN"]},{"cell_type":"markdown","metadata":{"id":"o3BNsIfK22bt"},"source":["## Defining Generator-network (from a trained ACGAN)"]},{"cell_type":"code","metadata":{"id":"gwVesc9m206r","executionInfo":{"status":"ok","timestamp":1639097677084,"user_tz":-540,"elapsed":512,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}}},"source":["def initialize_weights(net):\n","    for m in net.modules():\n","        if isinstance(m, nn.Conv2d):\n","            m.weight.data.normal_(0, 0.02)\n","            m.bias.data.zero_()\n","        elif isinstance(m, nn.ConvTranspose2d):\n","            m.weight.data.normal_(0, 0.02)\n","            m.bias.data.zero_()\n","        elif isinstance(m, nn.Linear):\n","            m.weight.data.normal_(0, 0.02)\n","            m.bias.data.zero_()\n","\n","class generator(nn.Module):\n","    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n","    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n","    def __init__(self, input_dim=100, output_dim=1, input_size=32, class_num=10):\n","        super(generator, self).__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.input_size = input_size\n","        self.class_num = class_num\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(self.input_dim + self.class_num, 1024),\n","            nn.BatchNorm1d(1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 128 * (self.input_size // 4) * (self.input_size // 4)),\n","            nn.BatchNorm1d(128 * (self.input_size // 4) * (self.input_size // 4)),\n","            nn.ReLU(),\n","        )\n","        self.deconv = nn.Sequential(\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n","            nn.Tanh(),\n","        )\n","        initialize_weights(self)\n","\n","    def forward(self, input, label):\n","        x = torch.cat([input, label], 1)\n","        x = self.fc(x)\n","        x = x.view(-1, 128, (self.input_size // 4), (self.input_size // 4))\n","        x = self.deconv(x)\n","\n","        return x\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oC1oQTVe2-Qa"},"source":["## Generate AM with GAN"]},{"cell_type":"code","metadata":{"id":"NgMwNsgo-AXb","executionInfo":{"status":"ok","timestamp":1639098459377,"user_tz":-540,"elapsed":2089,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}}},"source":["use_cuda = torch.cuda.is_available()\n","use_cuda = False\n","\n","class RegularizedClassSpecificImageGeneration():\n","    \"\"\"\n","        Produces an image that maximizes a certain class with gradient ascent. Uses Gaussian blur, weight decay, and clipping. \n","    \"\"\"\n","\n","    def __init__(self, model, target_class):\n","        #self.mean = [-0.485, -0.456, -0.406]\n","        #self.std = [1/0.229, 1/0.224, 1/0.225]\n","        self.model = model.cuda() if use_cuda else model\n","        self.model.eval()\n","        self.target_class = target_class\n","        # Generate a random image\n","        self.created_image = np.uint8(np.random.uniform(0, 255, (224, 224, 3)))\n","        #self.created_image = np.uint8(selected_mean_image*255)\n","        #######BEGIN GAN######\n","        # Generate class-vector for GAN:\n","        self.y = torch.tensor([target_class,target_class])\n","        self.y_vec_ = torch.zeros((2, 10)).scatter_(1, self.y.type(torch.LongTensor).unsqueeze(1), 1)\n","        # Generate random z-vector\n","        #self.z = torch.rand((1, 62))\n","        self.z = torch.rand((2, 62))\n","        #Init GAN\n","        self.G = generator(input_dim=62, output_dim=3, input_size=112)\n","        self.G.load_state_dict(torch.load(\"/content/drive/MyDrive/DLSP/models/FinalModels/Gan/ACGAN_G.pkl\",map_location=torch.device('cpu')))\n","        self.G.eval()\n","        #######END GAN########\n","        # Create the folder to export images if not exists\n","        if not os.path.exists(f'/content/drive/MyDrive/DLSP/generated_R2/class_{self.target_class}'):\n","            os.makedirs(f'/content/drive/MyDrive/DLSP/generated_R2/class_{self.target_class}')\n","\n","    def generate(self, iterations=150, blur_freq=4, blur_rad=1, wd=0.0001, clipping_value=0.1):\n","        \"\"\"Generates class specific image with enhancements to improve image quality. \n","        See https://arxiv.org/abs/1506.06579 for details on each argument's effect on output quality. \n","        #ORIGINAL iterations=150, blur_freq=4, blur_rad=1, wd=0.0001, clipping_value=0.1\n","        Play around with combinations of arguments. Besides the defaults, this combination has produced good images:\n","        blur_freq=6, blur_rad=0.8, wd = 0.05\n","        Keyword Arguments:\n","            iterations {int} -- Total iterations for gradient ascent (default: {150})\n","            blur_freq {int} -- Frequency of Gaussian blur effect, in iterations (default: {6})\n","            blur_rad {float} -- Radius for gaussian blur, passed to PIL.ImageFilter.GaussianBlur() (default: {0.8})\n","            wd {float} -- Weight decay value for Stochastic Gradient Ascent (default: {0.05})\n","            clipping_value {None or float} -- Value for gradient clipping (default: {0.1})\n","        \n","        Returns:\n","            np.ndarray -- Final maximally activated class image\n","        \"\"\"\n","        initial_learning_rate = 6\n","        for i in range(1, iterations):\n","            #Manipulating image\n","            self.img = self.G(self.z,self.y_vec_)\n","            self.img = self.img[0,:,:,:]\n","            #print(self.img.shape)\n","            self.img_np = self.img.squeeze().detach().numpy()\n","            self.img_np = np.swapaxes(np.swapaxes(self.img_np,0,1),1,2)\n","            self.img_np = np.uint8(np.clip(self.img_np,0,1)*255)\n","            self.img_pil = Image.fromarray(self.img_np)\n","            self.img_pil = self.img_pil.resize((224,224),Image.BICUBIC)\n","            self.im_as_arr = np.float32(self.img_pil)\n","            self.im_as_arr = self.im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n","            # Convert to float tensor\n","            self.im_as_ten = torch.from_numpy(self.im_as_arr).float()\n","            # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n","            self.created_image = self.im_as_ten.unsqueeze_(0)\n","            self.x0 = self.created_image.clone()\n","            self.created_image = Variable(self.created_image,requires_grad=True)\n","\n","\n","            #implement gaussian blurring every ith iteration\n","            #to improve output\n","            '''\n","            if i % blur_freq == 0:\n","                self.processed_image = preprocess_and_blur_image(\n","                    self.created_image, False, blur_rad)\n","            else:\n","                self.processed_image = preprocess_and_blur_image(\n","                    self.created_image, False)\n","            '''\n","            \n","            if use_cuda:\n","                self.created_image = self.created_image.cuda()\n","                print(\"using cuda\")\n","            # Define optimizer for the image - use weight decay to add regularization\n","            # in SGD, wd = 2 * L2 regularization (https://bbabenko.github.io/weight-decay/)\n","            optimizer = SGD([self.created_image],\n","                            lr=initial_learning_rate)\n","            # Forward\n","            output = self.model(self.created_image)\n","            # Target specific class\n","            class_loss = -output[0, self.target_class]\n","\n","            if i in np.linspace(0, iterations, 10, dtype=int):\n","                print('Iteration:', str(i), 'Loss',\n","                      \"{0:.2f}\".format(class_loss.data.cpu().numpy()))\n","                #print(self.z)\n","            # Zero grads\n","            self.model.zero_grad()\n","            # Backward\n","            class_loss.backward()\n","            '''\n","            if clipping_value:\n","                torch.nn.utils.clip_grad_norm(\n","                    self.model.parameters(), clipping_value)\n","            '''\n","            # Update image\n","            optimizer.step()\n","            \n","            #MAKE GENERATOR STEP\n","            self.z = generator_step(self.G, self.created_image, self.z, self.y_vec_, initial_learning_rate)\n","\n","\n","            # Recreate image\n","            #self.created_image = recreate_image(self.created_image.cpu())\n","            \n","            if i in np.linspace(0, iterations, 10, dtype=int):\n","                # Save image\n","                im_path = f'/content/drive/MyDrive/DLSP/generated_R2/class_{self.target_class}/c_{self.target_class}_iter_{i}_loss_{class_loss.data.cpu().numpy()}.jpg'\n","                save_image(self.created_image, im_path)\n","\n","        #save final image\n","        im_path = f'/content/drive/MyDrive/DLSP/generated_R2/class_{self.target_class}/c_{self.target_class}_iter_{i}_loss_{class_loss.data.cpu().numpy()}.jpg'\n","        save_image(self.created_image, im_path)\n","\n","        #write file with regularization details\n","        with open(f'/content/drive/MyDrive/DLSP/generated_R2/class_{self.target_class}/run_details.txt', 'w') as f:\n","            f.write(f'Iterations: {iterations}\\n')\n","            f.write(f'Blur freq: {blur_freq}\\n')\n","            f.write(f'Blur radius: {blur_rad}\\n')\n","            f.write(f'Weight decay: {wd}\\n')\n","            f.write(f'Clip value: {clipping_value}\\n')\n","\n","        #rename folder path with regularization details for easy access\n","        os.rename(f'/content/drive/MyDrive/DLSP/generated_R2/class_{self.target_class}',\n","                  f'/content/drive/MyDrive/DLSP/generated_R2/class_{self.target_class}_blurfreq_{blur_freq}_blurrad_{blur_rad}_wd{wd}')\n","        return self.created_image\n","\n","\n","def generator_step(G, x, oldz, y,lr):\n","    z = oldz\n","    z = Variable(z, requires_grad = True)\n","    #splitting z\n","    #z0 = z[0,:].unsqueeze(0)\n","    #z1 = z[1,:].unsqueeze(0)\n","    #setting optimizer\n","    #z0 = Variable(z0, requires_grad = True)\n","    optimizer = torch.optim.SGD([z], lr=0.00001)\n","    # run z through generator\n","    #x0 = G(torch.concat([z0,z1]),y)\n","    x0 = G(z,y)\n","    #fetching first image (batch of 2)\n","    img = x0[0,:,:,:]\n","    #Do image manipulation to input x get same size as output from generator (from 224 -> 112)\n","    # Same manipulation as getting output from generator to same size as inpu to CNN (from 112 -> 224)\n","    x_np = x.squeeze().detach().numpy()\n","    x_np = np.swapaxes(np.swapaxes(x_np,0,1),1,2)\n","    x_np = np.uint8(np.clip(x_np,0,1)*255)\n","    x_pil = Image.fromarray(x_np)\n","    x_pil = x_pil.resize((112,112),Image.BICUBIC)\n","    x_as_arr = np.float32(x_pil)\n","    x_as_arr = x_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n","    # Convert to float tensor\n","    x_as_ten = torch.from_numpy(x_as_arr).float()\n","    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n","    created_x = x_as_ten.unsqueeze_(0)\n","    x = created_x\n","    loss= ((img-x)**2).sum()\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    #Making sure Z-vec is between 0,1\n","    #z0 = torch.clamp(z0,0,1)\n","    z = torch.clamp(z,0,1)\n","    return z\n","    \n","def preprocess_and_blur_image(pil_im, resize_im=True, blur_rad=None):\n","    \"\"\"\n","        Processes image with optional Gaussian blur for CNNs\n","    Args:\n","        PIL_img (PIL_img): PIL Image or numpy array to process\n","        resize_im (bool): Resize to 224 or not\n","        blur_rad (int): Pixel radius for Gaussian blurring (default = None)\n","    returns:\n","        im_as_var (torch variable): Variable that contains processed float tensor\n","    \"\"\"\n","    # mean and std list for channels (Imagenet)\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","\n","    #ensure or transform incoming image to PIL image\n","    if type(pil_im) != Image.Image:\n","        try:\n","            pil_im = Image.fromarray(pil_im)\n","        except Exception as e:\n","            print(\n","                \"could not transform PIL_img to a PIL Image object. Please check input.\")\n","\n","    # Resize image\n","    if resize_im:\n","        pil_im.thumbnail((224, 224))\n","\n","    #add gaussin blur to image\n","    if blur_rad:\n","        pil_im = pil_im.filter(ImageFilter.GaussianBlur(blur_rad))\n","\n","    im_as_arr = np.float32(pil_im)\n","    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n","    # Normalize the channels\n","    for channel, _ in enumerate(im_as_arr):\n","        im_as_arr[channel] /= 255\n","        im_as_arr[channel] -= mean[channel]\n","        im_as_arr[channel] /= std[channel]\n","    # Convert to float tensor\n","    im_as_ten = torch.from_numpy(im_as_arr).float()\n","    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n","    im_as_ten.unsqueeze_(0)\n","    # Convert to Pytorch variable\n","    if use_cuda:\n","        im_as_var = Variable(im_as_ten.cuda(), requires_grad=True)\n","    else:\n","        im_as_var = Variable(im_as_ten, requires_grad=True)\n","    return im_as_var\n","\n","#target_class = 0\n","model = models.resnet50(pretrained=True)\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 10)\n","######LOADING IN WEIGHTS#######\n","resume = \"\"\n","resume = \"/content/drive/MyDrive/DLSP/models/FinalModels/Train2/018.ckpt\"\n","if resume:\n","    ckpt = torch.load(resume, map_location=torch.device('cpu'))\n","    model.load_state_dict(ckpt['net_state_dict'])\n","#target_class = 130  # Flamingo\n","#pretrained_model = models.alexnet(pretrained=True)\n","#csig = RegularizedClassSpecificImageGeneration(model, target_class)\n","#csig.generate()"],"execution_count":16,"outputs":[]},{"cell_type":"code","source":["for target_class in [0,1,2,3,4,5,6,7,8,9]:\n","  csig = RegularizedClassSpecificImageGeneration(model, target_class)\n","  csig.generate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSMgvoKG16zx","executionInfo":{"status":"ok","timestamp":1639100275169,"user_tz":-540,"elapsed":1804752,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"163d962e-8f9d-4e34-90a6-27b7a379ccca"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 16 Loss -157.02\n","Iteration: 33 Loss -165.71\n","Iteration: 50 Loss -190.93\n","Iteration: 66 Loss -170.86\n","Iteration: 83 Loss -167.97\n","Iteration: 100 Loss -174.19\n","Iteration: 116 Loss -165.87\n","Iteration: 133 Loss -179.31\n","Iteration: 16 Loss -326.62\n","Iteration: 33 Loss -326.55\n","Iteration: 50 Loss -326.32\n","Iteration: 66 Loss -326.24\n","Iteration: 83 Loss -326.24\n","Iteration: 100 Loss -326.24\n","Iteration: 116 Loss -326.24\n","Iteration: 133 Loss -326.24\n","Iteration: 16 Loss 45.58\n","Iteration: 33 Loss 47.90\n","Iteration: 50 Loss 44.74\n","Iteration: 66 Loss 45.87\n","Iteration: 83 Loss 35.92\n","Iteration: 100 Loss 45.02\n","Iteration: 116 Loss 47.69\n","Iteration: 133 Loss 44.85\n","Iteration: 16 Loss -155.87\n","Iteration: 33 Loss -158.87\n","Iteration: 50 Loss -158.49\n","Iteration: 66 Loss -158.90\n","Iteration: 83 Loss -159.01\n","Iteration: 100 Loss -158.75\n","Iteration: 116 Loss -157.13\n","Iteration: 133 Loss -157.07\n","Iteration: 16 Loss 128.57\n","Iteration: 33 Loss 128.51\n","Iteration: 50 Loss 128.66\n","Iteration: 66 Loss 128.62\n","Iteration: 83 Loss 128.33\n","Iteration: 100 Loss 128.65\n","Iteration: 116 Loss 128.62\n","Iteration: 133 Loss 128.54\n","Iteration: 16 Loss 161.84\n","Iteration: 33 Loss 140.66\n","Iteration: 50 Loss 162.46\n","Iteration: 66 Loss 165.02\n","Iteration: 83 Loss 144.59\n","Iteration: 100 Loss 166.15\n","Iteration: 116 Loss 169.41\n","Iteration: 133 Loss 144.10\n","Iteration: 16 Loss 142.99\n","Iteration: 33 Loss 152.48\n","Iteration: 50 Loss 142.66\n","Iteration: 66 Loss 151.26\n","Iteration: 83 Loss 126.59\n","Iteration: 100 Loss 145.96\n","Iteration: 116 Loss 148.87\n","Iteration: 133 Loss 135.23\n","Iteration: 16 Loss 345.68\n","Iteration: 33 Loss 345.70\n","Iteration: 50 Loss 345.55\n","Iteration: 66 Loss 345.59\n","Iteration: 83 Loss 345.40\n","Iteration: 100 Loss 345.36\n","Iteration: 116 Loss 345.61\n","Iteration: 133 Loss 345.38\n","Iteration: 16 Loss 132.33\n","Iteration: 33 Loss 132.48\n","Iteration: 50 Loss 132.58\n","Iteration: 66 Loss 132.72\n","Iteration: 83 Loss 132.83\n","Iteration: 100 Loss 132.87\n","Iteration: 116 Loss 132.85\n","Iteration: 133 Loss 132.86\n","Iteration: 16 Loss -173.75\n","Iteration: 33 Loss -171.91\n","Iteration: 50 Loss -164.07\n","Iteration: 66 Loss -161.85\n","Iteration: 83 Loss -176.28\n","Iteration: 100 Loss -176.62\n","Iteration: 116 Loss -167.95\n","Iteration: 133 Loss -178.01\n"]}]}]}