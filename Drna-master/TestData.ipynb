{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TestData.ipynb","provenance":[],"collapsed_sections":["rLG5Rx7tMPGT"],"authorship_tag":"ABX9TyPOsr7MtyaN3IAhC+q+Nqfy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a2750425b65a41dea3add409462181a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dc3e2813f2454644ba752229d59f1fc0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e915a94a1b954d0bbbb30f709deac0be","IPY_MODEL_54d285cfc4c34a84b23e4f64a1466efe","IPY_MODEL_2d3d38ead9f14967a7775438d7877966"]}},"dc3e2813f2454644ba752229d59f1fc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e915a94a1b954d0bbbb30f709deac0be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fe45500b864d423e955697813f3394f0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3b32a410f8904008a1ed16b6e9592fe5"}},"54d285cfc4c34a84b23e4f64a1466efe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_02cb0f8c5cfe49bb8f3e68fd5d88aa6e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":102530333,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":102530333,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c7cebb23764e487ba85be8c5e71f8c0a"}},"2d3d38ead9f14967a7775438d7877966":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6b7f13a3cae946dda6715ef0855ed507","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97.8M/97.8M [00:01&lt;00:00, 87.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f6045078d3aa4f138f6097aa825377d2"}},"fe45500b864d423e955697813f3394f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3b32a410f8904008a1ed16b6e9592fe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"02cb0f8c5cfe49bb8f3e68fd5d88aa6e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c7cebb23764e487ba85be8c5e71f8c0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b7f13a3cae946dda6715ef0855ed507":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f6045078d3aa4f138f6097aa825377d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"mDOdbok4zVE4"},"source":["# Colab Setup"]},{"cell_type":"code","metadata":{"id":"GaPpoYZSzX_o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638252838682,"user_tz":-540,"elapsed":30966,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"6d0e5f08-82d6-4169-ad05-05931edb759f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSV78ENQzZvq","executionInfo":{"status":"ok","timestamp":1638252842154,"user_tz":-540,"elapsed":290,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"144b5d76-e2cb-49c9-f514-e0711b8473d3"},"source":["\"\"\"\n","Change directory to where this file is located\n","\"\"\"\n","%cd '/content/drive/MyDrive/DLSP/Drna-master'"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/DLSP/Drna-master\n"]}]},{"cell_type":"markdown","metadata":{"id":"buswrFfwHWvb"},"source":["## Data Loader"]},{"cell_type":"code","metadata":{"id":"hSha0fN1DS9B","executionInfo":{"status":"ok","timestamp":1638252852176,"user_tz":-540,"elapsed":7129,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}}},"source":["import torch\n","import torch.utils.data as data\n","from torchvision import transforms\n","from PIL import Image\n","import sys\n","from config import INPUT_SIZE\n","\n","class GetLoader(data.Dataset):\n","    def __init__(self, data_root, data_list, train = None, train_procent = 0.8, transform=None):\n","        self.transform = transform\n","        self.data_root = data_root\n","       # print(self.data_root)\n","        f = open(data_list, 'r')\n","        data_list = f.readlines()\n","        f.close()\n","\n","        self.n_data = len(data_list)\n","\n","        self.img_paths = []\n","        self.labels = []\n","        self.intlabels = {\"Food\": 0, \"Clothes\": 1, \"Institution\": 2, \"Accessories\": 3, \"Transportation\": 4, \"Electronic\": 5, \"Necessities\": 6, \"Cosmetic\": 7, \"Leisure\": 8, \"Medical\": 9}\n","        for data in data_list:\n","            image_path = data[:-1]\n","            label = image_path.split('/')[0]\n","            self.img_paths.append(image_path)\n","            self.labels.append(label)\n","        \n","\n","    def __getitem__(self, item):\n","        img_path, label= self.img_paths[item], self.labels[item]\n","        label = self.intlabels[label]\n","        img_path_full = self.data_root+img_path\n","        img = Image.open(img_path_full).convert('RGB')\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        return img, label\n","\n","    def __len__(self):\n","        return self.n_data\n","\n","class TransformData(data.Dataset):\n","    def __init__(self, data, transform=None):\n","        self.data = data\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        x, y = self.data[index]\n","        if self.transform:\n","            x = self.transform(x)\n","        return x, y\n","    \n","    def __len__(self):\n","        return len(self.data)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rb2XHJrWMKe0","executionInfo":{"status":"ok","timestamp":1638252857175,"user_tz":-540,"elapsed":297,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}},"outputId":"d6a91712-fbe9-46d8-c163-0857ea0ff420"},"source":["torch.cuda.is_available()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"Uw6O13Q3CfM3","executionInfo":{"status":"ok","timestamp":1638252858997,"user_tz":-540,"elapsed":250,"user":{"displayName":"Mads Jungersen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15054749878452554668"}}},"source":["from tqdm import tqdm\n","from time import sleep"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rLG5Rx7tMPGT"},"source":["## Train RESNET50 fine-tuning all parameters (Large Images)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223,"referenced_widgets":["a2750425b65a41dea3add409462181a1","dc3e2813f2454644ba752229d59f1fc0","e915a94a1b954d0bbbb30f709deac0be","54d285cfc4c34a84b23e4f64a1466efe","2d3d38ead9f14967a7775438d7877966","fe45500b864d423e955697813f3394f0","3b32a410f8904008a1ed16b6e9592fe5","02cb0f8c5cfe49bb8f3e68fd5d88aa6e","c7cebb23764e487ba85be8c5e71f8c0a","6b7f13a3cae946dda6715ef0855ed507","f6045078d3aa4f138f6097aa825377d2"]},"id":"sr_rMntWMY4Z","outputId":"1c8703d2-5f98-483c-8e06-03e1a07041b8"},"source":["import os\n","import torch.utils.data\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import transforms\n","from torch.nn import DataParallel\n","from datetime import datetime\n","from torch.optim.lr_scheduler import MultiStepLR\n","from config import BATCH_SIZE, PROPOSAL_NUM, SAVE_FREQ, LR, WD, resume, save_dir, INPUT_SIZE, INPUT_SIZE_s, VAL_PROCENT, RESIZED_SIZE, RESIZED_SIZE_s\n","from torch import nn\n","from core import resnet as model\n","#from core import model_vgg as model\n","from core import data_loader\n","from core.utils import init_log, progress_bar\n","from PIL import Image\n","import numpy as np\n","\n","BATCH_SIZE = 6\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","start_epoch = 1\n","save_dir = os.path.join(save_dir, datetime.now().strftime('%Y%m%d_%H%M%S'))\n","if os.path.exists(save_dir):\n","    raise NameError('model dir exists!')\n","os.makedirs(save_dir)\n","print(save_dir)\n","logging = init_log(save_dir)\n","_print = logging.info\n","\n","################################################ DATA #############################\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize(RESIZED_SIZE, Image.BILINEAR),\n","    transforms.RandomCrop(INPUT_SIZE),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                 std=(0.229, 0.224, 0.225))\n","    ])\n","\n","\n","test_transform = transforms.Compose([\n","    transforms.Resize(RESIZED_SIZE, Image.BILINEAR),\n","    transforms.CenterCrop(INPUT_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                 std=(0.229, 0.224, 0.225))\n","    ])\n","\n","\n","\n","\n","Full_train_dataset = GetLoader(data_root='./data/train/',\n","                    data_list='./data/List/train_images_root.txt',\n","                    transform=None)\n","train_nr = len(Full_train_dataset)\n","train_dataset, val_dataset = torch.utils.data.random_split(Full_train_dataset,\n","                                               [train_nr - int(np.ceil(train_nr*(VAL_PROCENT))), int(np.ceil(train_nr*(VAL_PROCENT)))])\n","\n","train_dataset =  TransformData(train_dataset, transform = train_transform)\n","val_dataset =  TransformData(val_dataset, transform = test_transform)\n","\n","\n","test_dataset = GetLoader(data_root='./data/test/',\n","                data_list='./data/List/test_images_root.txt',\n","                transform=test_transform)\n","\n","trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                                          shuffle=True, num_workers=2, drop_last=False)\n","\n","validationloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                                          shuffle=False, num_workers=2, drop_last=False)\n","\n","\n","testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,\n","                                         shuffle=False, num_workers=2, drop_last=False)\n","\n","print(\"Number of training data:\", len(train_dataset))\n","print(\"Number of validation data:\", len(val_dataset))\n","print(\"Number of test data:\", len(test_dataset))\n","\n","################################################ DATA #############################\n","\n","# define model\n","net = torchvision.models.resnet50(pretrained=True)\n","#net = model.resnet152(pretrained=True)\n","# CHANING THE LAST FC-LAYER TO FIT 10 CLASSES\n","num_ftrs = net.fc.in_features\n","net.fc = nn.Linear(num_ftrs, 10)\n","\n","#resume = \"../models/20211123_010154/002.ckpt\"\n","if resume:\n","    ckpt = torch.load(resume)\n","    net.load_state_dict(ckpt['net_state_dict'])\n","    start_epoch = ckpt['epoch'] + 1\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.SGD(net.parameters(), lr = 0.01)\n","net = net.cuda()\n","net = DataParallel(net)\n","\n","for epoch in range(start_epoch, 6):\n","    # begin training\n","    print('--' * 50)\n","    net.train()\n","    train_loss = 0\n","    train_correct = 0\n","    total = 0\n","    with tqdm(trainloader, unit=\"batch\") as tepoch:\n","        for data in tepoch:\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","\n","            img, label = data[0].cuda(), data[1].cuda()\n","            batch_size = img.size(0)\n","            optimizer.zero_grad()\n","            outputs = net(img)\n","            loss = criterion(outputs,label)\n","            loss.backward()\n","            optimizer.step()\n","            # calculate accuracy\n","            _, predict = torch.max(outputs, 1)\n","            total += batch_size\n","            train_correct += torch.sum(predict.data == label.data)\n","            train_loss += loss.item() * batch_size\n","            tepoch.set_postfix(loss=train_loss/total, accuracy=100. * float(train_correct)/total)\n","            sleep(0.1)\n","    # Printing status after each epoch and perform validation    \n","    if epoch % SAVE_FREQ == 0:\n","        train_acc = float(train_correct) / total\n","        train_loss = train_loss / total\n","\n","        print(\n","            'epoch:{} - train loss: {:.3f} and train acc: {:.3f} total sample: {}'.format(\n","                epoch,\n","                train_loss,\n","                train_acc,\n","                total))\n","            \n","\t      # evaluate on test set\n","        test_loss = 0\n","        test_correct = 0\n","        total = 0\n","        net.eval()\n","        for i, data in enumerate(validationloader):\n","            img, label = data[0].cuda(), data[1].cuda()\n","            batch_size = img.size(0)\n","            \n","            outputs = net(img)\n","            loss = criterion(outputs,label)\n","            \n","            \n","            # calculate accuracy\n","            _, predict = torch.max(outputs, 1)\n","            total += batch_size\n","            test_correct += torch.sum(predict.data == label.data)\n","            test_loss += loss.item() * batch_size\n","\n","        test_acc = float(test_correct) / total\n","        test_loss = test_loss / total\n","        print(\n","            'epoch:{} - test loss: {:.3f} and test acc: {:.3f} total sample: {}'.format(\n","                epoch,\n","                test_loss,\n","                test_acc,\n","                total))\n","        \n","\n","\t      # save model\n","        net_state_dict = net.module.state_dict()\n","        if not os.path.exists(save_dir):\n","            os.mkdir(save_dir)\n","        torch.save({\n","            'epoch': epoch,\n","            'train_loss': train_loss,\n","            'train_acc': train_acc,\n","            'test_loss': test_loss,\n","            'test_acc': test_acc,\n","            'net_state_dict': net_state_dict},\n","            os.path.join(save_dir, '%03d.ckpt' % epoch))\n","\n","print('finishing training')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["../models/20211129_024953\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"]},{"output_type":"stream","name":"stdout","text":["Number of training data: 93566\n","Number of validation data: 23392\n","Number of test data: 50182\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2750425b65a41dea3add409462181a1","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 15595/15595 [3:24:48<00:00,  1.27batch/s, accuracy=36.8, loss=1.85]"]},{"output_type":"stream","name":"stdout","text":["epoch:1 - train loss: 1.846 and train acc: 0.368 total sample: 93566\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"RkFgOxKLeDQb"},"source":["## Train RESNET50 fine-tuning last layer (Small Images)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cn5PJzZpeDQc","outputId":"9e33edfb-0b16-4c1e-82b4-41cb30010016"},"source":["import os\n","import torch.utils.data\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import transforms\n","from torch.nn import DataParallel\n","from datetime import datetime\n","from torch.optim.lr_scheduler import MultiStepLR\n","from config import BATCH_SIZE, PROPOSAL_NUM, SAVE_FREQ, LR, WD, resume, save_dir, INPUT_SIZE, INPUT_SIZE_s, VAL_PROCENT, RESIZED_SIZE, RESIZED_SIZE_s\n","from torch import nn\n","#from core import resnet as model\n","#from core import model_vgg as model\n","#from core import data_loader\n","#from core.utils import init_log, progress_bar\n","from PIL import Image\n","import numpy as np\n","\n","BATCH_SIZE = 2\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","start_epoch = 1\n","save_dir = os.path.join(save_dir, datetime.now().strftime('%Y%m%d_%H%M%S'))\n","if os.path.exists(save_dir):\n","    raise NameError('model dir exists!')\n","os.makedirs(save_dir)\n","print(save_dir)\n","#logging = init_log(save_dir)\n","#_print = logging.info\n","\n","################################################ DATA #############################\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize(RESIZED_SIZE_s, Image.BILINEAR),\n","    transforms.RandomCrop(INPUT_SIZE_s),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                 std=(0.229, 0.224, 0.225))\n","    ])\n","\n","\n","test_transform = transforms.Compose([\n","    transforms.Resize(RESIZED_SIZE_s, Image.BILINEAR),\n","    transforms.CenterCrop(INPUT_SIZE_s),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                 std=(0.229, 0.224, 0.225))\n","    ])\n","\n","\n","\n","\n","Full_train_dataset = GetLoader(data_root='./data/train/',\n","                    data_list='./data/List/train_images_root.txt',\n","                    transform=None)\n","train_nr = len(Full_train_dataset)\n","train_dataset, val_dataset = torch.utils.data.random_split(Full_train_dataset,\n","                                               [train_nr - int(np.ceil(train_nr*(VAL_PROCENT))), int(np.ceil(train_nr*(VAL_PROCENT)))])\n","\n","train_dataset =  TransformData(train_dataset, transform = train_transform)\n","val_dataset =  TransformData(val_dataset, transform = test_transform)\n","\n","\n","test_dataset = GetLoader(data_root='./data/test/',\n","                data_list='./data/List/test_images_root.txt',\n","                transform=test_transform)\n","\n","trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                                          shuffle=True, num_workers=2, drop_last=False)\n","\n","validationloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                                          shuffle=False, num_workers=2, drop_last=False)\n","\n","\n","testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,\n","                                         shuffle=False, num_workers=2, drop_last=False)\n","\n","print(\"Number of training data:\", len(train_dataset))\n","print(\"Number of validation data:\", len(val_dataset))\n","print(\"Number of test data:\", len(test_dataset))\n","\n","################################################ DATA #############################\n","\n","# define model\n","net = torchvision.models.resnet50(pretrained=True)\n","#net = model.resnet152(pretrained=True)\n","# CHANING THE LAST FC-LAYER TO FIT 10 CLASSES\n","num_ftrs = net.fc.in_features\n","net.fc = nn.Linear(num_ftrs, 10)\n","\n","#resume = \"../models/20211123_010154/002.ckpt\"\n","if resume:\n","    ckpt = torch.load(resume)\n","    net.load_state_dict(ckpt['net_state_dict'])\n","    start_epoch = ckpt['epoch'] + 1\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.SGD(net.parameters(), lr = 0.01, momentum=0.9, weight_decay=WD)\n","scheduler = MultiStepLR(optimizer, milestones=[8, 20], gamma=0.1)\n","net = net.cuda()\n","net = DataParallel(net)\n","\n","for epoch in range(start_epoch, 20):\n","    # begin training\n","    print('--' * 50)\n","    net.train()\n","    train_loss = 0\n","    train_correct = 0\n","    total = 0\n","    with tqdm(trainloader, unit=\"batch\") as tepoch:\n","        for data in tepoch:\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","\n","            img, label = data[0].cuda(), data[1].cuda()\n","            batch_size = img.size(0)\n","            optimizer.zero_grad()\n","            outputs = net(img)\n","            loss = criterion(outputs,label)\n","            loss.backward()\n","            optimizer.step()\n","            # calculate accuracy\n","            _, predict = torch.max(outputs, 1)\n","            total += batch_size\n","            train_correct += torch.sum(predict.data == label.data)\n","            train_loss += loss.item() * batch_size\n","            tepoch.set_postfix(loss=train_loss/total, accuracy=100. * float(train_correct)/total)\n","            sleep(0.1)\n","    scheduler.step()\n","    # Printing status after each epoch and perform validation    \n","    if epoch % SAVE_FREQ == 0:\n","        train_acc = float(train_correct) / total\n","        train_loss = train_loss / total\n","\n","        print(\n","            'epoch:{} - train loss: {:.3f} and train acc: {:.3f} total sample: {}'.format(\n","                epoch,\n","                train_loss,\n","                train_acc,\n","                total))\n","            \n","\t      # evaluate on test set\n","        test_loss = 0\n","        test_correct = 0\n","        total = 0\n","        net.eval()\n","        for i, data in enumerate(validationloader):\n","            img, label = data[0].cuda(), data[1].cuda()\n","            batch_size = img.size(0)\n","            \n","            outputs = net(img)\n","            loss = criterion(outputs,label)\n","            \n","            \n","            # calculate accuracy\n","            _, predict = torch.max(outputs, 1)\n","            total += batch_size\n","            test_correct += torch.sum(predict.data == label.data)\n","            test_loss += loss.item() * batch_size\n","\n","        test_acc = float(test_correct) / total\n","        test_loss = test_loss / total\n","        print(\n","            'epoch:{} - test loss: {:.3f} and test acc: {:.3f} total sample: {}'.format(\n","                epoch,\n","                test_loss,\n","                test_acc,\n","                total))\n","        \n","\n","\t      # save model\n","        net_state_dict = net.module.state_dict()\n","        if not os.path.exists(save_dir):\n","            os.mkdir(save_dir)\n","        torch.save({\n","            'epoch': epoch,\n","            'train_loss': train_loss,\n","            'train_acc': train_acc,\n","            'test_loss': test_loss,\n","            'test_acc': test_acc,\n","            'net_state_dict': net_state_dict},\n","            os.path.join(save_dir, '%03d.ckpt' % epoch))\n","\n","print('finishing training')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["../models/20211130_061609\n","Number of training data: 93566\n","Number of validation data: 23392\n","Number of test data: 50182\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 46783/46783 [3:28:47<00:00,  3.73batch/s, accuracy=32.3, loss=2.02]"]},{"output_type":"stream","name":"stdout","text":["epoch:1 - train loss: 2.020 and train acc: 0.323 total sample: 93566\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}